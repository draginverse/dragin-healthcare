{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SGc50CeThZEw",
        "outputId": "e5ceaef2-0067-45d5-9a50-a0340bfda958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.10.0 httpx-sse-0.4.0 langchain_community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 pypdf-5.4.0 python-dotenv-1.1.0 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain openai faiss-cpu tiktoken pypdf langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MKPGyBSDi4be",
        "outputId": "a79f8480-0ce4-4d44-8ce6-c6d741e4f9af"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Load medical documents\n",
        "docs_path = \"../docs\"\n",
        "documents = []\n",
        "\n",
        "for file in os.listdir(docs_path):\n",
        "    file_path = os.path.join(docs_path, file)\n",
        "\n",
        "    # Load PDFs with PyPDFLoader\n",
        "    if file.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "    # Load text files with TextLoader\n",
        "    elif file.endswith(\".txt\"):\n",
        "        loader = TextLoader(file_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "# Split into smaller chunks for better retrieval\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Loaded {len(chunks)} document chunks successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jq\n",
            "  Downloading jq-1.8.0-cp38-cp38-win_amd64.whl.metadata (7.2 kB)\n",
            "Downloading jq-1.8.0-cp38-cp38-win_amd64.whl (420 kB)\n",
            "   ---------------------------------------- 0.0/420.0 kB ? eta -:--:--\n",
            "    --------------------------------------- 10.2/420.0 kB ? eta -:--:--\n",
            "    --------------------------------------- 10.2/420.0 kB ? eta -:--:--\n",
            "   - -------------------------------------- 20.5/420.0 kB 72.6 kB/s eta 0:00:06\n",
            "   -- ------------------------------------ 30.7/420.0 kB 145.2 kB/s eta 0:00:03\n",
            "   -- ------------------------------------ 30.7/420.0 kB 145.2 kB/s eta 0:00:03\n",
            "   ----- --------------------------------- 61.4/420.0 kB 192.5 kB/s eta 0:00:02\n",
            "   ----- --------------------------------- 61.4/420.0 kB 192.5 kB/s eta 0:00:02\n",
            "   ----- --------------------------------- 61.4/420.0 kB 192.5 kB/s eta 0:00:02\n",
            "   ------ -------------------------------- 71.7/420.0 kB 157.1 kB/s eta 0:00:03\n",
            "   -------- ------------------------------ 92.2/420.0 kB 174.7 kB/s eta 0:00:02\n",
            "   -------- ------------------------------ 92.2/420.0 kB 174.7 kB/s eta 0:00:02\n",
            "   ------------- ------------------------ 153.6/420.0 kB 248.0 kB/s eta 0:00:02\n",
            "   -------------------- ----------------- 225.3/420.0 kB 335.5 kB/s eta 0:00:01\n",
            "   -------------------------------------  419.8/420.0 kB 609.5 kB/s eta 0:00:01\n",
            "   -------------------------------------- 420.0/420.0 kB 570.0 kB/s eta 0:00:00\n",
            "Installing collected packages: jq\n",
            "Successfully installed jq-1.8.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install jq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 447 JSON document chunks successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import JSONLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import os\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "# Load JSON documents\n",
        "kb_chunks_path = \"../docs/kb_chunks\"\n",
        "kb_documents = []\n",
        "\n",
        "for file in os.listdir(kb_chunks_path):\n",
        "    file_path = os.path.join(kb_chunks_path, file)\n",
        "\n",
        "    if file.endswith(\".json\"):\n",
        "        loader = JSONLoader(\n",
        "            file_path=file_path,\n",
        "            jq_schema=\".content\",  # <- récupère uniquement le champ \"content\"\n",
        "            text_content=False     # <- retourne des objets Document, pas juste le texte brut\n",
        "        )\n",
        "        kb_documents.extend(loader.load())\n",
        "\n",
        "# Split JSON documents into smaller chunks\n",
        "chunks = text_splitter.split_documents(kb_documents)\n",
        "\n",
        "print(f\"Loaded {len(chunks)} JSON document chunks successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4XESreqFjzcu"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers pymilvus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DyJHu9US-Gfj"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient, DataType\n",
        "\n",
        "CLUSTER_ENDPOINT = \"https://in03-22706440ab83439.serverless.gcp-us-west1.cloud.zilliz.com\"\n",
        "TOKEN = \"fd3df03a845da1e42df2a2975f1d4733f62b044415d021f9f6047ef1f631504509c1a870e1370ebc8587c04883fae0c72c886204\"\n",
        "\n",
        "# 1. Set up a Milvus client\n",
        "client = MilvusClient(\n",
        "    uri=CLUSTER_ENDPOINT,\n",
        "    token=TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mlThMDvGjn-T"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Milvus\n",
        "from pymilvus import Collection, FieldSchema, CollectionSchema, DataType, connections, utility\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"document_embeddings\"\n",
        "\n",
        "# Connect to Milvus (ensure you've set up the connection correctly)\n",
        "connections.connect(\"default\", uri=CLUSTER_ENDPOINT, token=TOKEN)\n",
        "\n",
        "# # List all collections in the connected Milvus instance\n",
        "# existing_collections = utility.list_collections()\n",
        "\n",
        "# # Check if the collection exists or create it\n",
        "# if collection_name not in existing_collections:\n",
        "#     # Define the schema and create the collection\n",
        "#     embedding_dim = 768  # Example dimension, make sure this matches your model\n",
        "\n",
        "#     fields = [\n",
        "#         FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n",
        "#         FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
        "#     ]\n",
        "#     schema = CollectionSchema(fields, description=\"Document embeddings for retrieval\")\n",
        "\n",
        "#     # Create the collection\n",
        "#     collection = Collection(name=collection_name, schema=schema)\n",
        "# else:\n",
        "#     collection = Collection(collection_name)\n",
        "\n",
        "# print(f\"Collection '{collection_name}' is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection 'document_embeddings' has been deleted.\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import Collection\n",
        "\n",
        "# Load the collection\n",
        "collection = Collection(name=collection_name)\n",
        "\n",
        "# Drop the collection\n",
        "collection.drop()\n",
        "print(f\"Collection '{collection_name}' has been deleted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ygKZB7z-Gfk",
        "outputId": "4f1988ed-98c2-407d-a3a5-5342e5f49764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully connected to Milvus!\n"
          ]
        }
      ],
      "source": [
        "# from pymilvus import connections\n",
        "\n",
        "# # Disconnect from all existing connections\n",
        "# connections.disconnect(\"default\")\n",
        "\n",
        "# # Now try to connect again\n",
        "# connections.connect(\"default\", uri=CLUSTER_ENDPOINT, token=TOKEN)\n",
        "# print(\"Successfully connected to Milvus!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bHc6LD5ci3x3",
        "outputId": "04f22e91-75b7-47ea-eed6-926c17cc68e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\weszi\\AppData\\Local\\Temp\\ipykernel_3864\\2060263944.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "c:\\Users\\weszi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "# Load your local embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'auto_id': True, 'description': '', 'fields': [{'name': 'source', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'page', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': False}\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import Collection\n",
        "\n",
        "collection = Collection(name=collection_name)\n",
        "print(collection.schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " '10',\n",
              " '11',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '18',\n",
              " '19',\n",
              " '20',\n",
              " '21',\n",
              " '22',\n",
              " '23',\n",
              " '24',\n",
              " '25',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '30',\n",
              " '31',\n",
              " '32',\n",
              " '33',\n",
              " '34',\n",
              " '35',\n",
              " '36',\n",
              " '37',\n",
              " '38',\n",
              " '39',\n",
              " '40',\n",
              " '41',\n",
              " '42',\n",
              " '43',\n",
              " '44',\n",
              " '45',\n",
              " '46',\n",
              " '47',\n",
              " '48',\n",
              " '49',\n",
              " '50',\n",
              " '51',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '55',\n",
              " '56',\n",
              " '57',\n",
              " '58',\n",
              " '59',\n",
              " '60',\n",
              " '61',\n",
              " '62',\n",
              " '63',\n",
              " '64',\n",
              " '65',\n",
              " '66',\n",
              " '67',\n",
              " '68',\n",
              " '69',\n",
              " '70',\n",
              " '71',\n",
              " '72',\n",
              " '73',\n",
              " '74',\n",
              " '75',\n",
              " '76',\n",
              " '77',\n",
              " '78',\n",
              " '79',\n",
              " '80',\n",
              " '81',\n",
              " '82',\n",
              " '83',\n",
              " '84',\n",
              " '85',\n",
              " '86',\n",
              " '87',\n",
              " '88',\n",
              " '89',\n",
              " '90',\n",
              " '91',\n",
              " '92',\n",
              " '93',\n",
              " '94',\n",
              " '95',\n",
              " '96',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '100',\n",
              " '101',\n",
              " '102',\n",
              " '103',\n",
              " '104',\n",
              " '105',\n",
              " '106',\n",
              " '107',\n",
              " '108',\n",
              " '109',\n",
              " '110',\n",
              " '111',\n",
              " '112',\n",
              " '113',\n",
              " '114',\n",
              " '115',\n",
              " '116',\n",
              " '117',\n",
              " '118',\n",
              " '119',\n",
              " '120',\n",
              " '121',\n",
              " '122',\n",
              " '123',\n",
              " '124',\n",
              " '125',\n",
              " '126',\n",
              " '127',\n",
              " '128',\n",
              " '129',\n",
              " '130',\n",
              " '131',\n",
              " '132',\n",
              " '133',\n",
              " '134',\n",
              " '135',\n",
              " '136',\n",
              " '137',\n",
              " '138',\n",
              " '139',\n",
              " '140',\n",
              " '141',\n",
              " '142',\n",
              " '143',\n",
              " '144',\n",
              " '145',\n",
              " '146',\n",
              " '147',\n",
              " '148',\n",
              " '149',\n",
              " '150',\n",
              " '151',\n",
              " '152',\n",
              " '153',\n",
              " '154',\n",
              " '155',\n",
              " '156',\n",
              " '157',\n",
              " '158',\n",
              " '159',\n",
              " '160',\n",
              " '161',\n",
              " '162',\n",
              " '163',\n",
              " '164',\n",
              " '165',\n",
              " '166',\n",
              " '167',\n",
              " '168',\n",
              " '169',\n",
              " '170',\n",
              " '171',\n",
              " '172',\n",
              " '173',\n",
              " '174',\n",
              " '175',\n",
              " '176',\n",
              " '177',\n",
              " '178',\n",
              " '179',\n",
              " '180',\n",
              " '181',\n",
              " '182',\n",
              " '183',\n",
              " '184',\n",
              " '185',\n",
              " '186',\n",
              " '187',\n",
              " '188',\n",
              " '189',\n",
              " '190',\n",
              " '191',\n",
              " '192',\n",
              " '193',\n",
              " '194',\n",
              " '195',\n",
              " '196',\n",
              " '197',\n",
              " '198',\n",
              " '199',\n",
              " '200',\n",
              " '201',\n",
              " '202',\n",
              " '203',\n",
              " '204',\n",
              " '205',\n",
              " '206',\n",
              " '207',\n",
              " '208',\n",
              " '209',\n",
              " '210',\n",
              " '211',\n",
              " '212',\n",
              " '213',\n",
              " '214',\n",
              " '215',\n",
              " '216',\n",
              " '217',\n",
              " '218',\n",
              " '219',\n",
              " '220',\n",
              " '221',\n",
              " '222',\n",
              " '223',\n",
              " '224',\n",
              " '225',\n",
              " '226',\n",
              " '227',\n",
              " '228',\n",
              " '229',\n",
              " '230',\n",
              " '231',\n",
              " '232',\n",
              " '233',\n",
              " '234',\n",
              " '235',\n",
              " '236',\n",
              " '237',\n",
              " '238',\n",
              " '239',\n",
              " '240',\n",
              " '241',\n",
              " '242',\n",
              " '243',\n",
              " '244',\n",
              " '245',\n",
              " '246',\n",
              " '247',\n",
              " '248',\n",
              " '249',\n",
              " '250',\n",
              " '251',\n",
              " '252',\n",
              " '253',\n",
              " '254',\n",
              " '255',\n",
              " '256',\n",
              " '257',\n",
              " '258',\n",
              " '259',\n",
              " '260',\n",
              " '261',\n",
              " '262',\n",
              " '263',\n",
              " '264',\n",
              " '265',\n",
              " '266',\n",
              " '267',\n",
              " '268',\n",
              " '269',\n",
              " '270',\n",
              " '271',\n",
              " '272',\n",
              " '273',\n",
              " '274',\n",
              " '275',\n",
              " '276',\n",
              " '277',\n",
              " '278',\n",
              " '279',\n",
              " '280',\n",
              " '281',\n",
              " '282',\n",
              " '283',\n",
              " '284',\n",
              " '285',\n",
              " '286',\n",
              " '287',\n",
              " '288',\n",
              " '289',\n",
              " '290',\n",
              " '291',\n",
              " '292',\n",
              " '293',\n",
              " '294',\n",
              " '295',\n",
              " '296',\n",
              " '297',\n",
              " '298',\n",
              " '299',\n",
              " '300',\n",
              " '301',\n",
              " '302',\n",
              " '303',\n",
              " '304',\n",
              " '305',\n",
              " '306',\n",
              " '307',\n",
              " '308',\n",
              " '309',\n",
              " '310',\n",
              " '311',\n",
              " '312',\n",
              " '313',\n",
              " '314',\n",
              " '315',\n",
              " '316',\n",
              " '317',\n",
              " '318',\n",
              " '319',\n",
              " '320',\n",
              " '321',\n",
              " '322',\n",
              " '323',\n",
              " '324',\n",
              " '325',\n",
              " '326',\n",
              " '327',\n",
              " '328',\n",
              " '329',\n",
              " '330',\n",
              " '331',\n",
              " '332',\n",
              " '333',\n",
              " '334',\n",
              " '335',\n",
              " '336',\n",
              " '337',\n",
              " '338',\n",
              " '339',\n",
              " '340',\n",
              " '341',\n",
              " '342',\n",
              " '343',\n",
              " '344',\n",
              " '345',\n",
              " '346',\n",
              " '347',\n",
              " '348',\n",
              " '349',\n",
              " '350',\n",
              " '351',\n",
              " '352',\n",
              " '353',\n",
              " '354',\n",
              " '355',\n",
              " '356',\n",
              " '357',\n",
              " '358',\n",
              " '359',\n",
              " '360',\n",
              " '361',\n",
              " '362',\n",
              " '363',\n",
              " '364',\n",
              " '365',\n",
              " '366',\n",
              " '367',\n",
              " '368',\n",
              " '369',\n",
              " '370',\n",
              " '371',\n",
              " '372',\n",
              " '373',\n",
              " '374',\n",
              " '375',\n",
              " '376',\n",
              " '377',\n",
              " '378',\n",
              " '379',\n",
              " '380',\n",
              " '381',\n",
              " '382',\n",
              " '383',\n",
              " '384',\n",
              " '385',\n",
              " '386',\n",
              " '387',\n",
              " '388',\n",
              " '389',\n",
              " '390',\n",
              " '391',\n",
              " '392',\n",
              " '393',\n",
              " '394',\n",
              " '395',\n",
              " '396',\n",
              " '397',\n",
              " '398',\n",
              " '399',\n",
              " '400',\n",
              " '401',\n",
              " '402',\n",
              " '403',\n",
              " '404',\n",
              " '405',\n",
              " '406',\n",
              " '407',\n",
              " '408',\n",
              " '409',\n",
              " '410',\n",
              " '411',\n",
              " '412',\n",
              " '413',\n",
              " '414',\n",
              " '415',\n",
              " '416',\n",
              " '417',\n",
              " '418',\n",
              " '419',\n",
              " '420',\n",
              " '421',\n",
              " '422',\n",
              " '423',\n",
              " '424',\n",
              " '425',\n",
              " '426',\n",
              " '427',\n",
              " '428',\n",
              " '429',\n",
              " '430',\n",
              " '431',\n",
              " '432',\n",
              " '433',\n",
              " '434',\n",
              " '435',\n",
              " '436',\n",
              " '437',\n",
              " '438',\n",
              " '439',\n",
              " '440',\n",
              " '441',\n",
              " '442',\n",
              " '443',\n",
              " '444',\n",
              " '445',\n",
              " '446']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.vectorstores import Milvus\n",
        "\n",
        "# Prépare les données selon le schéma\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "metadatas = [doc.metadata for doc in chunks]\n",
        "embs = embeddings.embed_documents(texts)  # attention : bien utiliser la même dimension que `vector`\n",
        "\n",
        "# Prépare chaque champ du schéma\n",
        "sources = [meta.get(\"source\", \"\") for meta in metadatas]\n",
        "pages = [meta.get(\"page\", 0) for meta in metadatas]\n",
        "\n",
        "# Structure finale (doit contenir 4 listes dans cet ordre)\n",
        "docs_to_insert = {\n",
        "    \"source\": sources,\n",
        "    \"page\": pages,\n",
        "    \"text\": texts,\n",
        "    \"vector\": embs\n",
        "}\n",
        "\n",
        "# Crée une instance de Milvus connectée à ta collection\n",
        "vector_store = Milvus(\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=collection_name,\n",
        "    connection_args={\n",
        "        \"uri\": CLUSTER_ENDPOINT,\n",
        "        \"token\": TOKEN,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Crée une liste d'ids uniques (par exemple, utilise un range pour chaque document)\n",
        "ids = [str(i) for i in range(len(texts))]  # IDs uniques sous forme de string\n",
        "\n",
        "# Conversion des pages en int pour correspondre au schéma Milvus\n",
        "for meta in metadatas:\n",
        "    if \"page\" in meta:\n",
        "        try:\n",
        "            meta[\"page\"] = int(meta[\"page\"])\n",
        "        except ValueError:\n",
        "            meta[\"page\"] = 0\n",
        "\n",
        "# Ajoute les documents en fournissant les ids\n",
        "vector_store.add_texts(\n",
        "    texts=texts,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kzraqAWK-Gfl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to insert batch starting at entity: 0/447\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Failed to connect to Milvus cluster: <DataNotMatchException: (code=1, message=The data doesn't match with schema fields, expect 4 list, got 3)>\n"
          ]
        }
      ],
      "source": [
        "# Store the document embeddings into Milvus\n",
        "try:\n",
        "    vector_store = Milvus.from_documents(\n",
        "        chunks,\n",
        "        embeddings,\n",
        "        collection_name=collection_name,\n",
        "        connection_args={\n",
        "            \"uri\": CLUSTER_ENDPOINT,\n",
        "            \"token\": TOKEN,  # omit if unused\n",
        "        }\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to connect to Milvus cluster: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gw1UP3s8mmLN",
        "outputId": "c722d164-f67f-41cb-e368-2f7d807bd16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Retrieved Documents:\n",
            "Source: ../docs\\3.pdf\n",
            "Home  Health  Conditions and Diseases\n",
            "Signs of Respiratory\n",
            "Distress\n",
            "Signs of Respiratory Distress\n",
            "Learning the signs of respiratory\n",
            "distress\n",
            "People having trouble breathing often show signs that they are having to work\n",
            "harder to breathe or are not getting enough oxygen, indicating respiratory\n",
            "dist\n",
            "\n",
            "\n",
            "Source: ../docs\\2.pdf\n",
            "Global Regions\n",
            "Donate\n",
            "Home / Health topics / Chronic respiratory diseases\n",
            "CreditsChronic respiratory diseases\n",
            "Chronic respiratory diseases (CRDs) a\u0000ect the airways and other structures of the lungs. Some of the most\n",
            "common are chronic obstructive pulmonary disease (COPD), asthma, occupational lung d\n",
            "\n",
            "\n",
            "Source: ../docs\\4.pdf\n",
            "For Everyone \n",
            "Situation Summary\n",
            "About Respiratory Illnesses\n",
            "Risk Factors\n",
            "Prevention\n",
            "Treatment\n",
            "Data and Trends\n",
            "Resources to Prepare for Flu, COVID-19, and RSV\n",
            "Respiratory Virus Guidance\n",
            "Health Care Providers \n",
            "Best Practices for Patient Care\n",
            "Talking with Patients\n",
            "  Sign up for Email Updates  \n",
            "ESPA\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the symptoms of asthma?\"\n",
        "retrieved_docs = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "print(\"Top Retrieved Documents:\")\n",
        "for doc in retrieved_docs:\n",
        "    # print the source\n",
        "    print(f\"Source: {doc.metadata['source']}\")\n",
        "    # print the first 300 characters of the page content\n",
        "    print(doc.page_content[:300])  # Print a snippet\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
