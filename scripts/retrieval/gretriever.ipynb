{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSolPT5W5zgwhIEZfenxOD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/draginverse/dragin-healthcare/blob/feature%2Fg-retriever/scripts/retrieval/gretriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pcst_fast\n",
        "!pip install torch_scatter -f https://data.pyg.org/\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6hUhz5AASGs",
        "outputId": "75acd5a1-ca5a-4dc7-d5dd-17013d9f062b",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting pcst_fast\n",
            "  Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting pybind11>=2.1.0 (from pcst_fast)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11, pcst_fast\n",
            "Successfully installed pcst_fast-1.0.10 pybind11-2.13.6\n",
            "Looking in links: https://data.pyg.org/\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=803e1674e02c75536da80267d88b4ae60dde2ca1def1eec73ac962402fabc28e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simulate graphs input"
      ],
      "metadata": {
        "id": "YRSVHQhphrbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_graphs = [\n",
        "    {\n",
        "        \"nodes\": [\"Lung\", \"Alveoli\", \"Oxygen\", \"Carbon dioxide\", \"Bloodstream\"],\n",
        "        \"edges\": [\n",
        "            (\"Lung\", \"contains\", \"Alveoli\"),\n",
        "            (\"Alveoli\", \"exchange\", \"Oxygen\"),\n",
        "            (\"Alveoli\", \"remove\", \"Carbon dioxide\"),\n",
        "            (\"Oxygen\", \"enters\", \"Bloodstream\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"nodes\": [\"Asthma\", \"Airways\", \"Inflammation\", \"Bronchodilator\"],\n",
        "        \"edges\": [\n",
        "            (\"Asthma\", \"affects\", \"Airways\"),\n",
        "            (\"Airways\", \"show\", \"Inflammation\"),\n",
        "            (\"Bronchodilator\", \"relieves\", \"Inflammation\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"nodes\": [\"Pneumonia\", \"Lung infection\", \"Bacteria\", \"Antibiotics\"],\n",
        "        \"edges\": [\n",
        "            (\"Pneumonia\", \"is a\", \"Lung infection\"),\n",
        "            (\"Lung infection\", \"caused by\", \"Bacteria\"),\n",
        "            (\"Antibiotics\", \"treat\", \"Bacteria\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"nodes\": [\"Chronic obstructive pulmonary disease (COPD)\", \"Smoking\", \"Shortness of breath\", \"Lung damage\"],\n",
        "        \"edges\": [\n",
        "            (\"Smoking\", \"leads to\", \"Chronic obstructive pulmonary disease (COPD)\"),\n",
        "            (\"Chronic obstructive pulmonary disease (COPD)\", \"causes\", \"Shortness of breath\"),\n",
        "            (\"Chronic obstructive pulmonary disease (COPD)\", \"results in\", \"Lung damage\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"nodes\": [\"Bronchitis\", \"Cough\", \"Mucus\", \"Viral infection\"],\n",
        "        \"edges\": [\n",
        "            (\"Viral infection\", \"can cause\", \"Bronchitis\"),\n",
        "            (\"Bronchitis\", \"leads to\", \"Cough\"),\n",
        "            (\"Cough\", \"produces\", \"Mucus\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"nodes\": [\"Pulmonary embolism\", \"Blood clot\", \"Lung arteries\", \"Chest pain\"],\n",
        "        \"edges\": [\n",
        "            (\"Blood clot\", \"blocks\", \"Lung arteries\"),\n",
        "            (\"Blood clot\", \"leads to\", \"Pulmonary embolism\"),\n",
        "            (\"Pulmonary embolism\", \"causes\", \"Chest pain\")\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "'''     THIS IS FOR TRYING TO BUILD THE GRAPHS\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from torch_geometric.data import Data\n",
        "from src.utils.lm_modeling import load_model, load_text2embedding\n",
        "\n",
        "def create_graphs_in_memory():\n",
        "    dataset = load_dataset(\"rmanluo/RoG-webqsp\")\n",
        "    dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
        "\n",
        "    model_name = 'sbert'\n",
        "    model, tokenizer, device = load_model[model_name]()\n",
        "    text2embedding = load_text2embedding[model_name]\n",
        "\n",
        "    graphs = []         # list of Data objects (PyG graphs)\n",
        "    textual_nodes = []  # list of pandas DataFrames (for node attributes)\n",
        "    textual_edges = []  # list of pandas DataFrames (for edge attributes)\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        nodes = {}\n",
        "        edges = []\n",
        "        for h, r, t in dataset[i]['graph']:\n",
        "            h, t = h.lower(), t.lower()\n",
        "            if h not in nodes:\n",
        "                nodes[h] = len(nodes)\n",
        "            if t not in nodes:\n",
        "                nodes[t] = len(nodes)\n",
        "            edges.append({'src': nodes[h], 'edge_attr': r, 'dst': nodes[t]})\n",
        "\n",
        "        nodes_df = pd.DataFrame(\n",
        "            [{'node_id': v, 'node_attr': k} for k, v in nodes.items()],\n",
        "            columns=['node_id', 'node_attr']\n",
        "        )\n",
        "        edges_df = pd.DataFrame(edges, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "        if len(nodes_df) == 0:\n",
        "            continue  # skip empty graphs\n",
        "\n",
        "        # embeddings\n",
        "        x = text2embedding(model, tokenizer, device, nodes_df.node_attr.tolist())\n",
        "        edge_attr = text2embedding(model, tokenizer, device, edges_df.edge_attr.tolist())\n",
        "        edge_index = torch.LongTensor([edges_df.src.tolist(), edges_df.dst.tolist()])\n",
        "\n",
        "        pyg_graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(nodes_df))\n",
        "\n",
        "        graphs.append(pyg_graph)\n",
        "        textual_nodes.append(nodes_df)\n",
        "        textual_edges.append(edges_df)\n",
        "\n",
        "    return graphs, textual_nodes, textual_edges\n",
        "'''"
      ],
      "metadata": {
        "id": "r5B2t8rAjyVr",
        "outputId": "7c9717af-cfec-428f-84dc-53fdbad22927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'     THIS IS FOR TRYING TO BUILD THE GRAPHS\\nimport pandas as pd\\nimport torch\\nfrom datasets import load_dataset, concatenate_datasets\\nfrom torch_geometric.data import Data\\nfrom src.utils.lm_modeling import load_model, load_text2embedding\\n\\ndef create_graphs_in_memory():\\n    dataset = load_dataset(\"rmanluo/RoG-webqsp\")\\n    dataset = concatenate_datasets([dataset[\\'train\\'], dataset[\\'validation\\'], dataset[\\'test\\']])\\n\\n    model_name = \\'sbert\\'\\n    model, tokenizer, device = load_model[model_name]()\\n    text2embedding = load_text2embedding[model_name]\\n\\n    graphs = []         # list of Data objects (PyG graphs)\\n    textual_nodes = []  # list of pandas DataFrames (for node attributes)\\n    textual_edges = []  # list of pandas DataFrames (for edge attributes)\\n\\n    for i in range(len(dataset)):\\n        nodes = {}\\n        edges = []\\n        for h, r, t in dataset[i][\\'graph\\']:\\n            h, t = h.lower(), t.lower()\\n            if h not in nodes:\\n                nodes[h] = len(nodes)\\n            if t not in nodes:\\n                nodes[t] = len(nodes)\\n            edges.append({\\'src\\': nodes[h], \\'edge_attr\\': r, \\'dst\\': nodes[t]})\\n\\n        nodes_df = pd.DataFrame(\\n            [{\\'node_id\\': v, \\'node_attr\\': k} for k, v in nodes.items()],\\n            columns=[\\'node_id\\', \\'node_attr\\']\\n        )\\n        edges_df = pd.DataFrame(edges, columns=[\\'src\\', \\'edge_attr\\', \\'dst\\'])\\n\\n        if len(nodes_df) == 0:\\n            continue  # skip empty graphs\\n\\n        # embeddings\\n        x = text2embedding(model, tokenizer, device, nodes_df.node_attr.tolist())\\n        edge_attr = text2embedding(model, tokenizer, device, edges_df.edge_attr.tolist())\\n        edge_index = torch.LongTensor([edges_df.src.tolist(), edges_df.dst.tolist()])\\n\\n        pyg_graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(nodes_df))\\n\\n        graphs.append(pyg_graph)\\n        textual_nodes.append(nodes_df)\\n        textual_edges.append(edges_df)\\n\\n    return graphs, textual_nodes, textual_edges\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def transform_graphs_to_pyg(toy_graphs, feature_dim=384):\n",
        "    # Collect all nodes and edge types\n",
        "    all_nodes = list({node for graph in toy_graphs for node in graph[\"nodes\"]})\n",
        "    all_edge_types = list({edge[1] for graph in toy_graphs for edge in graph[\"edges\"]})\n",
        "\n",
        "    # Create encoders\n",
        "    node_encoder = {name: idx for idx, name in enumerate(all_nodes)}\n",
        "    edge_type_encoder = {name: idx for idx, name in enumerate(all_edge_types)}\n",
        "\n",
        "    pyg_graphs = []\n",
        "    for graph in toy_graphs:\n",
        "        # Create node features (!!! random for now !!!)\n",
        "        num_nodes = len(graph[\"nodes\"])\n",
        "        x = torch.randn(num_nodes, feature_dim)\n",
        "\n",
        "        # Process edges\n",
        "        edge_list = []\n",
        "        edge_attr_list = []\n",
        "        for src, edge_type, dst in graph[\"edges\"]:\n",
        "            src_idx = graph[\"nodes\"].index(src)  # Local graph index\n",
        "            dst_idx = graph[\"nodes\"].index(dst)  # Local graph index\n",
        "            edge_list.append([src_idx, dst_idx])\n",
        "            edge_attr_list.append(edge_type_encoder[edge_type])\n",
        "\n",
        "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attr_list, dtype=torch.long)\n",
        "\n",
        "        pyg_graphs.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
        "\n",
        "    return pyg_graphs, node_encoder, edge_type_encoder\n",
        "\n",
        "# Transform the graphs\n",
        "pyg_graphs, node_encoder, edge_type_encoder = transform_graphs_to_pyg(toy_graphs)\n",
        "\n",
        "# Create a batch of graphs for processing\n",
        "toy_graph_batch = Batch.from_data_list(pyg_graphs)\n",
        "\n",
        "# Print information about the first graph\n",
        "print(\"First graph in PyG format:\")\n",
        "print(pyg_graphs[0])\n",
        "#print(\"\\nNode mapping:\", {name: idx for idx, name in enumerate(node_encoder.classes_)})\n",
        "#print(\"Edge type mapping:\", {name: idx for idx, name in enumerate(edge_type_encoder.classes_)})\n",
        "print(\"\\nBatch information:\")\n",
        "print(toy_graph_batch)\n",
        "print(\"Batch vector:\", toy_graph_batch.batch)\n",
        "\n",
        "print(\"Total nodes:\", toy_graph_batch.num_nodes)\n",
        "print(\"Batch vector max index:\", toy_graph_batch.batch.max())\n",
        "print(\"Batch vector length:\", len(toy_graph_batch.batch))\n",
        "\n",
        "# Verify consistency\n",
        "assert toy_graph_batch.batch.max() < len(pyg_graphs), \"Batch indices exceed graph count\"\n",
        "assert len(toy_graph_batch.batch) == toy_graph_batch.num_nodes, \"Batch vector length mismatch\""
      ],
      "metadata": {
        "id": "xVglUMKT1quZ",
        "outputId": "21cb7f72-b45a-4a57-d934-5213a54e02cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First graph in PyG format:\n",
            "Data(x=[5, 384], edge_index=[2, 4], edge_attr=[4])\n",
            "\n",
            "Batch information:\n",
            "DataBatch(x=[25, 384], edge_index=[2, 19], edge_attr=[19], batch=[25], ptr=[7])\n",
            "Batch vector: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5,\n",
            "        5])\n",
            "Total nodes: 25\n",
            "Batch vector max index: tensor(5)\n",
            "Batch vector length: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gnn.py"
      ],
      "metadata": {
        "id": "XJ1J2eLj_d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TransformerConv, GATConv\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GCN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(in_channels=in_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=out_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=4):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GATConv(hidden_channels, out_channels, heads=num_heads, concat=False))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x,edge_index=edge_index, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "load_gnn_model = {\n",
        "    'gcn': GCN,\n",
        "    'gat': GAT,\n",
        "    'gt': GraphTransformer,\n",
        "}"
      ],
      "metadata": {
        "id": "OKE0QK-R55bL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# graph_encoder.py"
      ],
      "metadata": {
        "id": "fzfEgi_l_mOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W9iAVCzK2cdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter\n",
        "#from gnn import load_gnn_model\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = load_gnn_model[args.gnn_model_name](\n",
        "            in_channels=args.gnn_in_dim,\n",
        "            out_channels=args.gnn_hidden_dim,\n",
        "            hidden_channels=args.gnn_hidden_dim,\n",
        "            num_layers=args.gnn_num_layers,\n",
        "            dropout=args.gnn_dropout,\n",
        "            num_heads=args.gnn_num_heads,\n",
        "        )\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(args.gnn_hidden_dim, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 4096),\n",
        "        )\n",
        "\n",
        "    def encode(self, graphs):\n",
        "        graphs = graphs.to(next(self.parameters()).device)\n",
        "        n_embeds, _ = self.graph_encoder(graphs.x, graphs.edge_index.long(), graphs.edge_attr)\n",
        "        g_embeds = scatter(n_embeds, graphs.batch, dim=0, reduce='mean')\n",
        "        projected_embeds = self.projector(g_embeds)\n",
        "        return projected_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pcst_retrieval.py"
      ],
      "metadata": {
        "id": "yShIE19f_sEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pcst_fast import pcst_fast\n",
        "from torch_geometric.data.data import Data\n",
        "\n",
        "\n",
        "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
        "    c = 0.01\n",
        "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
        "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "        graph = Data(x=graph.x, edge_index=graph.edge_index, edge_attr=graph.edge_attr, num_nodes=graph.num_nodes)\n",
        "        return graph, desc\n",
        "\n",
        "    root = -1  # unrooted\n",
        "    num_clusters = 1\n",
        "    pruning = 'gw'\n",
        "    verbosity_level = 0\n",
        "    if topk > 0:\n",
        "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.x)\n",
        "        topk = min(topk, graph.num_nodes)\n",
        "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
        "\n",
        "        n_prizes = torch.zeros_like(n_prizes)\n",
        "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
        "    else:\n",
        "        n_prizes = torch.zeros(graph.num_nodes)\n",
        "\n",
        "    if topk_e > 0:\n",
        "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.edge_attr)\n",
        "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
        "\n",
        "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
        "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
        "        last_topk_e_value = topk_e\n",
        "        for k in range(topk_e):\n",
        "            indices = e_prizes == topk_e_values[k]\n",
        "            value = min((topk_e-k)/sum(indices), last_topk_e_value)\n",
        "            e_prizes[indices] = value\n",
        "            last_topk_e_value = value*(1-c)\n",
        "        # reduce the cost of the edges such that at least one edge is selected\n",
        "        cost_e = min(cost_e, e_prizes.max().item()*(1-c/2))\n",
        "    else:\n",
        "        e_prizes = torch.zeros(graph.num_edges)\n",
        "\n",
        "    costs = []\n",
        "    edges = []\n",
        "    vritual_n_prizes = []\n",
        "    virtual_edges = []\n",
        "    virtual_costs = []\n",
        "    mapping_n = {}\n",
        "    mapping_e = {}\n",
        "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
        "        prize_e = e_prizes[i]\n",
        "        if prize_e <= cost_e:\n",
        "            mapping_e[len(edges)] = i\n",
        "            edges.append((src, dst))\n",
        "            costs.append(cost_e - prize_e)\n",
        "        else:\n",
        "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
        "            mapping_n[virtual_node_id] = i\n",
        "            virtual_edges.append((src, virtual_node_id))\n",
        "            virtual_edges.append((virtual_node_id, dst))\n",
        "            virtual_costs.append(0)\n",
        "            virtual_costs.append(0)\n",
        "            vritual_n_prizes.append(prize_e - cost_e)\n",
        "\n",
        "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
        "    num_edges = len(edges)\n",
        "    if len(virtual_costs) > 0:\n",
        "        costs = np.array(costs+virtual_costs)\n",
        "        edges = np.array(edges+virtual_edges)\n",
        "\n",
        "    vertices, edges = pcst_fast(edges, prizes, costs, root, num_clusters, pruning, verbosity_level)\n",
        "\n",
        "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
        "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
        "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "    if len(virtual_vertices) > 0:\n",
        "        virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
        "        selected_edges = np.array(selected_edges+virtual_edges)\n",
        "\n",
        "    edge_index = graph.edge_index[:, selected_edges]\n",
        "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    n = textual_nodes.iloc[selected_nodes]\n",
        "    e = textual_edges.iloc[selected_edges]\n",
        "    desc = n.to_csv(index=False)+'\\n'+e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
        "\n",
        "    x = graph.x[selected_nodes]\n",
        "    edge_attr = graph.edge_attr[selected_edges]\n",
        "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
        "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
        "    edge_index = torch.LongTensor([src, dst])\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
        "\n",
        "    return data, desc"
      ],
      "metadata": {
        "id": "DxoyW5_U7NUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieve_pipeline.py"
      ],
      "metadata": {
        "id": "4fV8lnTL_xCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "#from retrieval import retrieval_via_pcst\n",
        "\n",
        "# Step 1 — Encode the query into an embedding\n",
        "def encode_query(query_text, query_encoder):\n",
        "    # query_encoder must be a model that outputs a vector from text\n",
        "    with torch.no_grad():\n",
        "        q_emb = query_encoder(query_text)  # returns a (feature_dim,) tensor\n",
        "    return q_emb\n",
        "\n",
        "# Step 2 — Prepare the graph (already assumed to be in PyG Data format)\n",
        "def prepare_graph(graph_data):\n",
        "    # graph_data must be a torch_geometric.data.Data object\n",
        "    return graph_data\n",
        "\n",
        "# Step 3 — Prepare textual node and edge tables\n",
        "def prepare_textual_info(graph_data):\n",
        "    # Dummy example: create node and edge DataFrames\n",
        "    textual_nodes = pd.DataFrame({'node_feature': graph_data.x.tolist()})\n",
        "    textual_edges = pd.DataFrame({\n",
        "        'src': graph_data.edge_index[0].tolist(),\n",
        "        'dst': graph_data.edge_index[1].tolist(),\n",
        "        'edge_attr': graph_data.edge_attr.tolist()\n",
        "    })\n",
        "    return textual_nodes, textual_edges\n",
        "\n",
        "# Step 4 — Retrieval via PCST\n",
        "def retrieve_subgraph(query_text, query_encoder, graph_data):\n",
        "    q_emb = encode_query(query_text, query_encoder)\n",
        "    textual_nodes, textual_edges = prepare_textual_info(graph_data)\n",
        "    subgraph, description = retrieval_via_pcst(\n",
        "        graph=graph_data,\n",
        "        q_emb=q_emb,\n",
        "        textual_nodes=textual_nodes,\n",
        "        textual_edges=textual_edges,\n",
        "        topk=3,\n",
        "        topk_e=3,\n",
        "        cost_e=0.5\n",
        "    )\n",
        "    return subgraph, description\n",
        "\n",
        "# Example usage:\n",
        "# query_encoder = ... (your trained model or any text embedder)\n",
        "# graph_data = ... (your loaded graph)\n",
        "# subgraph, desc = retrieve_subgraph(\"example query\", query_encoder, graph_data)\n"
      ],
      "metadata": {
        "id": "oXBl4c8S_OxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compute graph embeddings & retrieve the most relevant graph:"
      ],
      "metadata": {
        "id": "_7VjFFCzulqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from types import SimpleNamespace\n",
        "#from src.model.graph_encoder import GraphEncoder\n",
        "#from src.model.gnn import load_gnn_model\n",
        "\n",
        "# 1. Load text encoder\n",
        "text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # Or another model\n",
        "\n",
        "# 2. Define GNN parameters\n",
        "graph_encoder_args = SimpleNamespace(\n",
        "    gnn_model_name=\"gat\",\n",
        "    gnn_in_dim=384,        # SBERT dim\n",
        "    gnn_hidden_dim=128,\n",
        "    gnn_num_layers=2,\n",
        "    gnn_dropout=0.1,\n",
        "    gnn_num_heads=4,       # Needed for GAT\n",
        ")\n",
        "\n",
        "# 3. Create the graph encoder\n",
        "graph_encoder = GraphEncoder(graph_encoder_args)\n",
        "\n",
        "graph_encoder.eval()  # Disable dropout during inference\n",
        "\n",
        "# 4. Encode all graphs into fixed-size graph representations\n",
        "'''graph_reprs = []\n",
        "for graph in toy_graph_batch:\n",
        "    with torch.no_grad():\n",
        "        repr = graph_encoder.encode(graph)  # add batch dimension if needed\n",
        "        graph_reprs.append(repr.squeeze(0))  # shape: [hidden_dim]\n",
        "\n",
        "graph_reprs = torch.stack(graph_reprs)  # shape: [num_graphs, hidden_dim]'''\n",
        "# ===\n",
        "\n",
        "# 4.1. Transform all graphs to PyG format\n",
        "pyg_graphs, _, _ = transform_graphs_to_pyg(toy_graphs,feature_dim=384)\n",
        "\n",
        "# 4.2. Create one batch containing all graphs\n",
        "batch = Batch.from_data_list(pyg_graphs)\n",
        "\n",
        "# 4.3. Encode all graphs in one forward pass (most efficient)\n",
        "with torch.no_grad():\n",
        "    graph_reprs = graph_encoder.encode(batch)\n",
        "# ===\n",
        "'''\n",
        "# Test each graph separately\n",
        "for i, data in enumerate(pyg_graphs):\n",
        "    try:\n",
        "        single_batch = Batch.from_data_list([data])\n",
        "        with torch.no_grad():\n",
        "            graph_encoder.encode(single_batch)\n",
        "        print(f\"Graph {i} processed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in graph {i}: {str(e)}\")'''\n",
        "\n",
        "# 5. Encode the query\n",
        "query = \"What are common treatments for asthma?\"\n",
        "q_emb = torch.tensor(text_encoder.encode(query, convert_to_tensor=True))  # shape: [hidden_dim]\n",
        "# Process query with projection\n",
        "text_projection = nn.Linear(384, 4096)\n",
        "with torch.no_grad():\n",
        "    q_emb = text_encoder.encode(query, convert_to_tensor=True)\n",
        "    q_emb = text_projection(q_emb)  # Now shape: [4096]\n",
        "\n",
        "# 6. Compare and select the most similar graph\n",
        "sims = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), graph_reprs)  # shape: [num_graphs]\n",
        "top_idx = torch.argmax(sims).item()\n",
        "\n",
        "# 7. Get the most relevant graph\n",
        "selected_graph = toy_graph_batch[top_idx]\n",
        "#selected_node_texts = toy_node_texts[top_idx]\n",
        "#selected_edge_texts = toy_edge_texts[top_idx]\n",
        "\n",
        "# Get the most similar graph\n",
        "print(f\"Most relevant graph index: {top_idx}\")\n",
        "matched_graph = toy_graphs[top_idx]\n",
        "# Print with formatting\n",
        "print(\"=== Most Similar Graph ===\")\n",
        "print(f\"Match Score: {sims[top_idx]:.3f}\")\n",
        "print(\"\\nNodes:\")\n",
        "print(\"- \" + \"\\n- \".join(matched_graph[\"nodes\"]))\n",
        "\n",
        "print(\"\\nEdges:\")\n",
        "for src, rel, dst in matched_graph[\"edges\"]:\n",
        "    print(f\"{src} -- {rel} --> {dst}\")\n"
      ],
      "metadata": {
        "id": "D_LCx18Zumdy",
        "outputId": "c81c70d2-ac5b-4d69-8c4b-df100c85604b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant graph index: 3\n",
            "=== Most Similar Graph ===\n",
            "Match Score: -0.010\n",
            "\n",
            "Nodes:\n",
            "- Chronic obstructive pulmonary disease (COPD)\n",
            "- Smoking\n",
            "- Shortness of breath\n",
            "- Lung damage\n",
            "\n",
            "Edges:\n",
            "Smoking -- leads to --> Chronic obstructive pulmonary disease (COPD)\n",
            "Chronic obstructive pulmonary disease (COPD) -- causes --> Shortness of breath\n",
            "Chronic obstructive pulmonary disease (COPD) -- results in --> Lung damage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-e41efb9612b0>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  q_emb = torch.tensor(text_encoder.encode(query, convert_to_tensor=True))  # shape: [hidden_dim]\n"
          ]
        }
      ]
    }
  ]
}