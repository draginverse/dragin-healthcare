{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgniXIH+1bMfHmqn9PK5fH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/draginverse/dragin-healthcare/blob/feature%2Fg-retriever/scripts/retrieval/gretriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pcst_fast\n",
        "!pip install torch_scatter -f https://data.pyg.org/\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6hUhz5AASGs",
        "outputId": "299b3ffa-c52d-496c-8ad4-df7587060405",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting pcst_fast\n",
            "  Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting pybind11>=2.1.0 (from pcst_fast)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11, pcst_fast\n",
            "Successfully installed pcst_fast-1.0.10 pybind11-2.13.6\n",
            "Looking in links: https://data.pyg.org/\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3622722 sha256=739b80d50f56f8a4b45ac78e9019cda49a15a5224b9176ddd8abbfcfe7fc40ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text embedder (384 to 4096)"
      ],
      "metadata": {
        "id": "ZRg80LE94fYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "text_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "text_model = AutoModel.from_pretrained(text_model_name)\n",
        "\n",
        "text_projection = nn.Linear(384, 4096)\n",
        "\n",
        "def text2embedding(texts, model, tokenizer, projection_layer):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Mean pooling over tokens\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Project to target dimension\n",
        "    projected = projection_layer(embeddings)\n",
        "\n",
        "    return projected"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7ENwDZu_1VEG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simulate graphs input"
      ],
      "metadata": {
        "id": "YRSVHQhphrbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_graphs = [\n",
        "    [\n",
        "        (\"asthma\", \"caused_by\", \"allergens\"),\n",
        "        (\"inhaler\", \"treats\", \"asthma\"),\n",
        "        (\"asthma\", \"symptom\", \"shortness of breath\")\n",
        "    ],\n",
        "    [\n",
        "        (\"asthma\", \"treated_by\", \"inhaler\"),\n",
        "        (\"inhaler\", \"treats\", \"asthma\"),\n",
        "        (\"asthma\", \"symptom\", \"shortness of breath\")\n",
        "    ],\n",
        "    [\n",
        "        (\"copd\", \"risk_factor\", \"smoking\"),\n",
        "        (\"oxygen therapy\", \"treats\", \"copd\"),\n",
        "        (\"copd\", \"symptom\", \"chronic cough\")\n",
        "    ],\n",
        "    [\n",
        "        (\"bronchitis\", \"caused_by\", \"virus\"),\n",
        "        (\"bronchitis\", \"symptom\", \"chest discomfort\"),\n",
        "        (\"rest\", \"helps_with\", \"bronchitis\")\n",
        "    ],\n",
        "    [\n",
        "        (\"pneumonia\", \"caused_by\", \"bacteria\"),\n",
        "        (\"antibiotics\", \"treats\", \"pneumonia\"),\n",
        "        (\"pneumonia\", \"symptom\", \"fever\")\n",
        "    ],\n",
        "    [\n",
        "        (\"covid-19\", \"affects\", \"lungs\"),\n",
        "        (\"vaccine\", \"prevents\", \"covid-19\"),\n",
        "        (\"covid-19\", \"symptom\", \"loss of smell\")\n",
        "    ]\n",
        "]\n"
      ],
      "metadata": {
        "id": "r5B2t8rAjyVr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform graphs to pyg (needed for the encoder)"
      ],
      "metadata": {
        "id": "sDfuGBlTPmjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "'''\n",
        "# 1. Load the MiniLM model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 2. Text to embedding function (optimized for MiniLM)\n",
        "def text2embedding(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                      max_length=128, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use mean pooling instead of CLS token for better performance\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu()\n",
        "    return embeddings\n",
        "'''\n",
        "# 3. Graph transformation function\n",
        "def transform_graphs_to_pyg(triple_graphs):\n",
        "    pyg_graphs = []\n",
        "\n",
        "    for triples in triple_graphs:\n",
        "        # Extract all node names\n",
        "        node_names = set()\n",
        "        for src, _, dst in triples:\n",
        "            node_names.update([src.lower(), dst.lower()])\n",
        "        node_names = sorted(node_names)\n",
        "        node_map = {name: idx for idx, name in enumerate(node_names)}\n",
        "\n",
        "        # Edge list and texts\n",
        "        edge_list = []\n",
        "        edge_texts = []\n",
        "        for src, rel, dst in triples:\n",
        "            edge_list.append([node_map[src.lower()], node_map[dst.lower()]])\n",
        "            edge_texts.append(f\"relation: {rel}\")\n",
        "\n",
        "        # Node embeddings\n",
        "        node_texts = [f\"node: {name}\" for name in node_names]\n",
        "        node_embeddings = text2embedding(node_texts, text_model, text_tokenizer, text_projection)\n",
        "        edge_embeddings = text2embedding(edge_texts, text_model, text_tokenizer, text_projection) if edge_texts else torch.zeros(0, 4096)\n",
        "\n",
        "        # Create PyG graph\n",
        "        pyg_graph = Data(\n",
        "            x=node_embeddings,\n",
        "            edge_index=torch.tensor(edge_list).t().contiguous(),\n",
        "            edge_attr=edge_embeddings,\n",
        "            num_nodes=len(node_names)\n",
        "        )\n",
        "        pyg_graphs.append(pyg_graph)\n",
        "\n",
        "    return pyg_graphs\n",
        "\n",
        "# ======== Verify consistency ===============\n",
        "# Transform the graphs\n",
        "pyg_graphs = transform_graphs_to_pyg(toy_graphs)\n",
        "\n",
        "# Create a batch of graphs for processing\n",
        "toy_graph_batch = Batch.from_data_list(pyg_graphs)\n",
        "\n",
        "# Print information about the first graph\n",
        "print(\"First graph in PyG format:\")\n",
        "print(pyg_graphs[0])\n",
        "#print(\"\\nNode mapping:\", {name: idx for idx, name in enumerate(node_encoder.classes_)})\n",
        "#print(\"Edge type mapping:\", {name: idx for idx, name in enumerate(edge_type_encoder.classes_)})\n",
        "print(\"\\nBatch information:\")\n",
        "print(toy_graph_batch)\n",
        "print(\"Batch vector:\", toy_graph_batch.batch)\n",
        "\n",
        "print(\"Total nodes:\", toy_graph_batch.num_nodes)\n",
        "print(\"Batch vector max index:\", toy_graph_batch.batch.max())\n",
        "print(\"Batch vector length:\", len(toy_graph_batch.batch))\n",
        "\n",
        "assert toy_graph_batch.batch.max() < len(pyg_graphs), \"Batch indices exceed graph count\"\n",
        "assert len(toy_graph_batch.batch) == toy_graph_batch.num_nodes, \"Batch vector length mismatch\""
      ],
      "metadata": {
        "id": "xVglUMKT1quZ",
        "outputId": "205ed588-10a2-474e-cb86-d3a13877e583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First graph in PyG format:\n",
            "Data(x=[4, 4096], edge_index=[2, 3], edge_attr=[3, 4096], num_nodes=4)\n",
            "\n",
            "Batch information:\n",
            "DataBatch(x=[23, 4096], edge_index=[2, 18], edge_attr=[18, 4096], num_nodes=23, batch=[23], ptr=[7])\n",
            "Batch vector: tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5])\n",
            "Total nodes: 23\n",
            "Batch vector max index: tensor(5)\n",
            "Batch vector length: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "34An1egkMHVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch_geometric.loader import DataLoader\n",
        "import copy\n",
        "\n",
        "# New external training utilities\n",
        "class GNNTrainingUtils:\n",
        "    @staticmethod\n",
        "    def train_model(model, train_graphs, train_targets, test_graphs, test_targets, config=None):\n",
        "        \"\"\"External training function that works with your original GraphEncoder\"\"\"\n",
        "        config = config or {\n",
        "            'batch_size': 32,\n",
        "            'epochs': 10,\n",
        "            'learning_rate': 1e-4,\n",
        "            'weight_decay': 1e-5\n",
        "        }\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_model = None\n",
        "\n",
        "\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            indices = torch.randperm(len(train_graphs))  # Random batch ordering\n",
        "            for i in range(0, len(indices), config['batch_size']):\n",
        "                batch_idx = indices[i:i+config['batch_size']].tolist()\n",
        "\n",
        "                # Create batch from selected graphs\n",
        "                batch_graphs = [train_graphs[idx] for idx in batch_idx]\n",
        "                batch = Batch.from_data_list(batch_graphs).to(device)\n",
        "                batch_targets = train_targets[batch_idx].clone().to(device)\n",
        "\n",
        "                # Training step\n",
        "                optimizer.zero_grad()\n",
        "                '''embeddings = model.encode(batch, training_mode=True)'''\n",
        "                embeddings = model.encode(batch)\n",
        "                loss = criterion(embeddings, batch_targets)\n",
        "                print(f\"Batch {i // config['batch_size']}: loss={loss.item()}\")\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Evaluation phase\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                test_batch = Batch.from_data_list(test_graphs).to(device)\n",
        "                '''embeddings = model.encode(test_batch,training_mode=True)'''\n",
        "                embeddings = model.encode(test_batch)\n",
        "                test_loss = criterion(embeddings, test_targets.to(device))\n",
        "\n",
        "            # Track best model\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                '''best_model = copy.deepcopy(model.state_dict())'''\n",
        "                best_model = model.state_dict()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {train_loss/(len(indices)/config['batch_size']):.4f}\")\n",
        "\n",
        "        # Load best model weights\n",
        "        model.load_state_dict(best_model)\n",
        "        return model\n",
        "\n",
        "    '''@staticmethod\n",
        "    def evaluate(model, data_loader, criterion, device):\n",
        "        \"\"\"Evaluation function\"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for graphs, targets in data_loader:\n",
        "                graphs, targets = graphs.to(device), targets.to(device)\n",
        "                outputs = model.encode(graphs, training_mode=True)\n",
        "                total_loss += criterion(outputs, targets).item()\n",
        "        return total_loss / len(data_loader)'''\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(model, path):\n",
        "        \"\"\"Save helper that works with your original class\"\"\"\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(model_class, args, path):\n",
        "        \"\"\"Load helper that works with your original class\"\"\"\n",
        "        model = model_class(args)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        return model"
      ],
      "metadata": {
        "id": "rZWottgYMKGE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gnn.py"
      ],
      "metadata": {
        "id": "XJ1J2eLj_d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TransformerConv, GATConv\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GCN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(in_channels=in_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=out_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=4):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GATConv(hidden_channels, out_channels, heads=num_heads, concat=False))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x,edge_index=edge_index, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "load_gnn_model = {\n",
        "    'gcn': GCN,\n",
        "    'gat': GAT,\n",
        "    'gt': GraphTransformer,\n",
        "}"
      ],
      "metadata": {
        "id": "OKE0QK-R55bL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# graph_encoder.py (embeds graphs)"
      ],
      "metadata": {
        "id": "fzfEgi_l_mOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "W9iAVCzK2cdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter\n",
        "#from gnn import load_gnn_model\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = load_gnn_model[args.gnn_model_name](\n",
        "            in_channels=args.gnn_in_dim,\n",
        "            out_channels=args.gnn_hidden_dim,\n",
        "            hidden_channels=args.gnn_hidden_dim,\n",
        "            num_layers=args.gnn_num_layers,\n",
        "            dropout=args.gnn_dropout,\n",
        "            num_heads=args.gnn_num_heads,\n",
        "        )\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(args.gnn_hidden_dim, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 4096),\n",
        "        )\n",
        "\n",
        "        # Add this adapter ONLY for training\n",
        "        self.training_adapter = nn.Linear(4096, 384)  # Projects to MiniLM dimension\n",
        "\n",
        "    def encode(self, graphs, training_mode=False):\n",
        "        graphs = graphs.to(next(self.parameters()).device)\n",
        "        n_embeds, _ = self.graph_encoder(graphs.x, graphs.edge_index.long(), graphs.edge_attr)\n",
        "        g_embeds = scatter(n_embeds, graphs.batch, dim=0, reduce='mean')\n",
        "        projected_embeds = self.projector(g_embeds)\n",
        "        # Only use adapter during training\n",
        "        if training_mode:\n",
        "            return self.training_adapter(projected_embeds)\n",
        "        return projected_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pcst_retrieval.py"
      ],
      "metadata": {
        "id": "yShIE19f_sEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pcst_fast import pcst_fast\n",
        "from torch_geometric.data.data import Data\n",
        "\n",
        "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
        "    c = 0.01\n",
        "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
        "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "        return graph, desc\n",
        "\n",
        "    # === Project graph features to match query dim ===\n",
        "    projection = nn.Linear(graph.x.size(1), q_emb.size(0), bias=False)\n",
        "    with torch.no_grad():\n",
        "        projected_node_x = projection(graph.x)           # shape: [num_nodes, 4096]\n",
        "        projected_edge_attr = projection(graph.edge_attr)  # shape: [num_edges, 4096]\n",
        "\n",
        "    # === Proceed with projected features ===\n",
        "    if topk > 0:\n",
        "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, projected_node_x)\n",
        "        topk = min(topk, graph.num_nodes)\n",
        "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
        "        n_prizes = torch.zeros_like(n_prizes)\n",
        "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
        "    else:\n",
        "        n_prizes = torch.zeros(graph.num_nodes)\n",
        "\n",
        "    if topk_e > 0:\n",
        "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, projected_edge_attr)\n",
        "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
        "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
        "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
        "        last_topk_e_value = topk_e\n",
        "        for k in range(topk_e):\n",
        "            indices = e_prizes == topk_e_values[k]\n",
        "            value = min((topk_e - k) / sum(indices), last_topk_e_value)\n",
        "            e_prizes[indices] = value\n",
        "            last_topk_e_value = value * (1 - c)\n",
        "        cost_e = min(cost_e, e_prizes.max().item() * (1 - c / 2))\n",
        "    else:\n",
        "        e_prizes = torch.zeros(graph.num_edges)\n",
        "\n",
        "    # === Rest of the PCST logic remains unchanged ===\n",
        "    costs = []\n",
        "    edges = []\n",
        "    vritual_n_prizes = []\n",
        "    virtual_edges = []\n",
        "    virtual_costs = []\n",
        "    mapping_n = {}\n",
        "    mapping_e = {}\n",
        "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
        "        prize_e = e_prizes[i]\n",
        "        if prize_e <= cost_e:\n",
        "            mapping_e[len(edges)] = i\n",
        "            edges.append((src, dst))\n",
        "            costs.append(cost_e - prize_e)\n",
        "        else:\n",
        "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
        "            mapping_n[virtual_node_id] = i\n",
        "            virtual_edges.append((src, virtual_node_id))\n",
        "            virtual_edges.append((virtual_node_id, dst))\n",
        "            virtual_costs.append(0)\n",
        "            virtual_costs.append(0)\n",
        "            vritual_n_prizes.append(prize_e - cost_e)\n",
        "\n",
        "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
        "    num_edges = len(edges)\n",
        "    if len(virtual_costs) > 0:\n",
        "        costs = np.array(costs + virtual_costs)\n",
        "        edges = np.array(edges + virtual_edges)\n",
        "\n",
        "    vertices, edges = pcst_fast(edges, prizes, costs, -1, 1, 'gw', 0)\n",
        "\n",
        "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
        "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
        "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "    if len(virtual_vertices) > 0:\n",
        "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
        "        selected_edges = np.array(selected_edges + virtual_edges)\n",
        "\n",
        "    edge_index = graph.edge_index[:, selected_edges]\n",
        "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    n = textual_nodes.iloc[selected_nodes]\n",
        "    e = textual_edges.iloc[selected_edges]\n",
        "    desc = n.to_csv(index=False) + '\\n' + e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
        "    x = graph.x[selected_nodes]\n",
        "    edge_attr = graph.edge_attr[selected_edges]\n",
        "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
        "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
        "    edge_index = torch.LongTensor([src, dst])\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
        "\n",
        "    return data, desc"
      ],
      "metadata": {
        "id": "DxoyW5_U7NUg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compute graph embeddings & retrieve the most relevant graph:"
      ],
      "metadata": {
        "id": "_7VjFFCzulqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train GNN before retrieving"
      ],
      "metadata": {
        "id": "7hJv53Nzpa8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Training\n",
        "# 2. Create targets (mean node features for each graph)\n",
        "targets = torch.stack([g.x.mean(dim=0) for g in pyg_graphs])  # Graph-level targets\n",
        "\n",
        "# 2. Split data (80% train, 20% test)\n",
        "train_graphs, test_graphs, train_targets, test_targets = train_test_split(\n",
        "    pyg_graphs, targets, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "graph_encoder_args = SimpleNamespace(\n",
        "        gnn_model_name=\"gat\",\n",
        "        gnn_in_dim=4096,\n",
        "        gnn_hidden_dim=128,\n",
        "        gnn_num_layers=2,\n",
        "        gnn_dropout=0.1,\n",
        "        gnn_num_heads=4,\n",
        "    )\n",
        "\n",
        "# Initialize fresh model\n",
        "graph_encoder = GraphEncoder(graph_encoder_args)\n",
        "\n",
        "# Train the model\n",
        "trained_encoder = GNNTrainingUtils.train_model(\n",
        "    graph_encoder,\n",
        "    train_graphs, train_targets,\n",
        "    test_graphs, test_targets,\n",
        "    config={\n",
        "        'batch_size': 2,\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 1e-4,\n",
        "        'weight_decay': 1e-5\n",
        "    }\n",
        ")\n",
        "\n",
        "# Final evaluation (example)\n",
        "'''\n",
        "test_batch = Batch.from_data_list(test_graphs).to(device)\n",
        "with torch.no_grad():\n",
        "    embeddings = trained_encoder.encode(test_batch)\n",
        "    test_loss = nn.MSELoss()(embeddings, test_targets.to(device))\n",
        "print(f\"\\nFinal Test Loss: {test_loss:.4f}\")'''\n",
        "\n",
        "# Save the trained model\n",
        "GNNTrainingUtils.save_model(trained_encoder, \"/content/pretrained.pth\")\n"
      ],
      "metadata": {
        "id": "-7WVAPKjSwnB",
        "outputId": "c24abb90-d4c8-4e86-fd61-e2f41a2718f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: loss=0.12044795602560043\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-870a24f50e9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m trained_encoder = GNNTrainingUtils.train_model(\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mgraph_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-f6b0bace1bcb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_graphs, train_targets, test_graphs, test_targets, config)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {i // config['batch_size']}: loss={loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from types import SimpleNamespace\n",
        "'''from sentence_transformers import SentenceTransformer'''\n",
        "#from src.model.graph_encoder import GraphEncoder\n",
        "#from src.model.gnn import load_gnn_model\n",
        "\n",
        "def retrieve_relevant_graph(graphs, query, topk=1):\n",
        "    # 1. Load text encoder\n",
        "    '''text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")'''\n",
        "\n",
        "    # 2. Define GNN parameters\n",
        "    graph_encoder_args = SimpleNamespace(\n",
        "        gnn_model_name=\"gat\",\n",
        "        gnn_in_dim=4096,\n",
        "        gnn_hidden_dim=128,\n",
        "        gnn_num_layers=2,\n",
        "        gnn_dropout=0.1,\n",
        "        gnn_num_heads=4,\n",
        "    )\n",
        "\n",
        "    # 3. Create the graph encoder\n",
        "    # graph_encoder = GraphEncoder(graph_encoder_args)\n",
        "    # or load a pretrained model\n",
        "    graph_encoder = GNNTrainingUtils.load_model(GraphEncoder, graph_encoder_args, \"/content/pretrained.pth\")\n",
        "\n",
        "    graph_encoder.eval()\n",
        "\n",
        "    # 4. Encode all graphs\n",
        "    pyg_graphs = transform_graphs_to_pyg(graphs)\n",
        "    batch = Batch.from_data_list(pyg_graphs)\n",
        "    with torch.no_grad():\n",
        "        graph_reprs = graph_encoder.encode(batch)  # shape: [num_graphs, 4096]\n",
        "\n",
        "    # 5. Encode query and project\n",
        "    q_emb = text2embedding([query],text_model,text_tokenizer,text_projection)\n",
        "    '''\n",
        "    text_projection = nn.Linear(384, 4096)\n",
        "    with torch.no_grad():\n",
        "        q_emb = text_encoder.encode(query, convert_to_tensor=True)\n",
        "        q_emb = text_projection(q_emb)  # shape: [4096]\n",
        "'''\n",
        "    # 6. Compute similarities\n",
        "    sims = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), graph_reprs)\n",
        "\n",
        "    # 7. Retrieve top-k indices and scores\n",
        "    top_scores, top_indices = torch.topk(sims, k=topk)\n",
        "    selected_graphs = [graphs[i] for i in top_indices.tolist()]\n",
        "\n",
        "    return selected_graphs, top_indices.tolist(), sims, q_emb\n"
      ],
      "metadata": {
        "id": "D_LCx18Zumdy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieve once"
      ],
      "metadata": {
        "id": "A6v3BdbZOqxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how can i treat asthma?\"\n",
        "# Get the most similar graph\n",
        "selected_graphs, top_indices, sims, q_emb = retrieve_relevant_graph(toy_graphs, query)\n",
        "#print(f\"Most relevant graph index: {top_idx}\")\n",
        "#print(selected_graph)\n",
        "\n",
        "matched_graph = toy_graphs[top_indices[0]]\n",
        "# Print with formatting\n",
        "print(\"=== Most Similar Graph ===\")\n",
        "print(f\"Match Score: {sims[top_indices[0]]:.3f}\")\n",
        "print(f\"Graph : {matched_graph}\")"
      ],
      "metadata": {
        "id": "7pd_TATIOfTF",
        "outputId": "803495a2-f135-453d-d5e9-289d108b0e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Most Similar Graph ===\n",
            "Match Score: 0.016\n",
            "Graph : [('covid-19', 'affects', 'lungs'), ('vaccine', 'prevents', 'covid-19'), ('covid-19', 'symptom', 'loss of smell')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieve 50x for testing"
      ],
      "metadata": {
        "id": "IX_vO8c_OmaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "query = \"how can i treat asthma?\"\n",
        "number_of_retrievals = 50\n",
        "top_k = 1\n",
        "\n",
        "# Run retrievals\n",
        "all_results = []\n",
        "for _ in range(number_of_retrievals):\n",
        "    _, top_indices, sims, _ = retrieve_relevant_graph(toy_graphs, query, topk=top_k)\n",
        "    for idx in top_indices:\n",
        "        all_results.append((idx, sims[idx].item()))\n",
        "\n",
        "# Analyze\n",
        "stats = defaultdict(list)\n",
        "for idx, score in all_results:\n",
        "    stats[idx].append(score)\n",
        "\n",
        "print(\"\\n=== Detailed Statistics ===\")\n",
        "for idx in sorted(stats.keys(), key=lambda x: -np.mean(stats[x])):\n",
        "    scores = stats[idx]\n",
        "    graph = toy_graphs[idx]\n",
        "    print(f\"\\nGraph {idx}:\")\n",
        "    print(f\"  Frequency: {len(scores)}/{number_of_retrievals * top_k}\")\n",
        "    print(f\"  Avg Score: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
        "    print(f\"  Preview  : {graph[0]}...\")"
      ],
      "metadata": {
        "id": "YUvKV2ijJbpV",
        "outputId": "0da3caf0-810a-4b25-85a6-c72561ece31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Detailed Statistics ===\n",
            "\n",
            "Graph 0:\n",
            "  Frequency: 1/50\n",
            "  Avg Score: 0.024 ± 0.000\n",
            "  Preview  : ('asthma', 'caused_by', 'allergens')...\n",
            "\n",
            "Graph 5:\n",
            "  Frequency: 3/50\n",
            "  Avg Score: 0.019 ± 0.003\n",
            "  Preview  : ('covid-19', 'affects', 'lungs')...\n",
            "\n",
            "Graph 2:\n",
            "  Frequency: 11/50\n",
            "  Avg Score: 0.014 ± 0.018\n",
            "  Preview  : ('copd', 'risk_factor', 'smoking')...\n",
            "\n",
            "Graph 4:\n",
            "  Frequency: 16/50\n",
            "  Avg Score: 0.010 ± 0.011\n",
            "  Preview  : ('pneumonia', 'caused_by', 'bacteria')...\n",
            "\n",
            "Graph 3:\n",
            "  Frequency: 13/50\n",
            "  Avg Score: 0.005 ± 0.017\n",
            "  Preview  : ('bronchitis', 'caused_by', 'virus')...\n",
            "\n",
            "Graph 1:\n",
            "  Frequency: 6/50\n",
            "  Avg Score: 0.003 ± 0.011\n",
            "  Preview  : ('asthma', 'treated_by', 'inhaler')...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract subgraph"
      ],
      "metadata": {
        "id": "HAsL9Xp1e7EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 4. Extract subgraphs via PCST\n",
        "selected_subgraphs = []\n",
        "descriptions = []\n",
        "topk_nodes=5\n",
        "topk_edges=3\n",
        "cost_e=0.5\n",
        "\n",
        "for idx in top_indices:\n",
        "    graph = pyg_graphs[idx]\n",
        "    triples = toy_graphs[idx]  # List of (src, rel, dst)\n",
        "\n",
        "    # Unique nodes\n",
        "    nodes = sorted(set(n.lower() for triple in triples for n in (triple[0], triple[2])))\n",
        "\n",
        "    textual_nodes = pd.DataFrame({'node': [f\"node: {name}\" for name in nodes]})\n",
        "    textual_edges = pd.DataFrame(triples, columns=[\"src\", \"edge_attr\", \"dst\"])\n",
        "    textual_edges[\"edge_attr\"] = textual_edges[\"edge_attr\"].apply(lambda x: f\"relation: {x}\")\n",
        "\n",
        "    subgraph, desc = retrieval_via_pcst(\n",
        "        graph, q_emb, textual_nodes, textual_edges,\n",
        "        topk=topk_nodes, topk_e=topk_edges, cost_e=cost_e\n",
        "    )\n",
        "    selected_subgraphs.append(subgraph)\n",
        "    descriptions.append(desc)\n",
        "\n",
        "print(selected_subgraphs)\n",
        "print(descriptions)\n",
        "\n",
        "print(\"\\n=== PCST Subgraph Summary ===\")\n",
        "print(f\"Nodes: {subgraph.num_nodes}\")\n",
        "print(f\"Edges: {subgraph.edge_index.size(1)}\")\n",
        "\n",
        "# Convert edge_index to readable form\n",
        "print(\"\\nSelected Triples:\")\n",
        "for i in range(subgraph.edge_index.size(1)):\n",
        "    src_idx = subgraph.edge_index[0, i].item()\n",
        "    dst_idx = subgraph.edge_index[1, i].item()\n",
        "    edge_vec = subgraph.edge_attr[i]\n",
        "    # Try to find matching textual triple (fallback to index if needed)\n",
        "    try:\n",
        "        src = textual_nodes.iloc[src_idx][\"node\"]\n",
        "        dst = textual_nodes.iloc[dst_idx][\"node\"]\n",
        "        rel = textual_edges.iloc[i][\"edge_attr\"]\n",
        "    except:\n",
        "        src, dst, rel = src_idx, dst_idx, \"[vector]\"\n",
        "    print(f\"  ({src}) --[{rel}]--> ({dst})\")\n",
        "\n",
        "# Print node names\n",
        "print(\"\\nIncluded Nodes:\")\n",
        "for i in range(subgraph.num_nodes):\n",
        "    node_name = textual_nodes.iloc[i][\"node\"]\n",
        "    print(f\"  - {node_name}\")"
      ],
      "metadata": {
        "id": "ynohb4xme-Ck",
        "outputId": "75b1fd55-fbb5-4c7c-d67a-856841e8573b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data(x=[1, 384], edge_index=[2, 0], edge_attr=[0, 384], num_nodes=1)]\n",
            "['node\\nnode: covid-19\\n\\nsrc,edge_attr,dst\\n']\n",
            "\n",
            "=== PCST Subgraph Summary ===\n",
            "Nodes: 1\n",
            "Edges: 0\n",
            "\n",
            "Selected Triples:\n",
            "\n",
            "Included Nodes:\n",
            "  - node: covid-19\n"
          ]
        }
      ]
    }
  ]
}