{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUNp9NWBJkb8CfxuIphcIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/draginverse/dragin-healthcare/blob/feature%2Fg-retriever/scripts/retrieval/gretriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pcst_fast\n",
        "!pip install torch_scatter -f https://data.pyg.org/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6hUhz5AASGs",
        "outputId": "71fcbe08-b7f6-44c8-f1a7-c3786395cde8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Requirement already satisfied: pcst_fast in /usr/local/lib/python3.11/dist-packages (1.0.10)\n",
            "Requirement already satisfied: pybind11>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pcst_fast) (2.13.6)\n",
            "Looking in links: https://data.pyg.org/\n",
            "Collecting torch_scatter\n",
            "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=3622717 sha256=0a3662d45106bcd147d48f0442da991e6ea4defb0afec0e41eb264c28201593e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gnn.py"
      ],
      "metadata": {
        "id": "XJ1J2eLj_d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TransformerConv, GATConv\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GCN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(in_channels=in_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=out_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=4):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GATConv(hidden_channels, out_channels, heads=num_heads, concat=False))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x,edge_index=edge_index, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "load_gnn_model = {\n",
        "    'gcn': GCN,\n",
        "    'gat': GAT,\n",
        "    'gt': GraphTransformer,\n",
        "}"
      ],
      "metadata": {
        "id": "OKE0QK-R55bL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# graph_encoder.py"
      ],
      "metadata": {
        "id": "fzfEgi_l_mOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W9iAVCzK2cdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter\n",
        "#from gnn import load_gnn_model\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = load_gnn_model[args.gnn_model_name](\n",
        "            in_channels=args.gnn_in_dim,\n",
        "            out_channels=args.gnn_hidden_dim,\n",
        "            hidden_channels=args.gnn_hidden_dim,\n",
        "            num_layers=args.gnn_num_layers,\n",
        "            dropout=args.gnn_dropout,\n",
        "            num_heads=args.gnn_num_heads,\n",
        "        )\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(args.gnn_hidden_dim, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 4096),\n",
        "        )\n",
        "\n",
        "    def encode(self, graphs):\n",
        "        graphs = graphs.to(next(self.parameters()).device)\n",
        "        n_embeds, _ = self.graph_encoder(graphs.x, graphs.edge_index.long(), graphs.edge_attr)\n",
        "        g_embeds = scatter(n_embeds, graphs.batch, dim=0, reduce='mean')\n",
        "        projected_embeds = self.projector(g_embeds)\n",
        "        return projected_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieval.py"
      ],
      "metadata": {
        "id": "yShIE19f_sEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pcst_fast import pcst_fast\n",
        "from torch_geometric.data.data import Data\n",
        "\n",
        "\n",
        "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
        "    c = 0.01\n",
        "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
        "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "        graph = Data(x=graph.x, edge_index=graph.edge_index, edge_attr=graph.edge_attr, num_nodes=graph.num_nodes)\n",
        "        return graph, desc\n",
        "\n",
        "    root = -1  # unrooted\n",
        "    num_clusters = 1\n",
        "    pruning = 'gw'\n",
        "    verbosity_level = 0\n",
        "    if topk > 0:\n",
        "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.x)\n",
        "        topk = min(topk, graph.num_nodes)\n",
        "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
        "\n",
        "        n_prizes = torch.zeros_like(n_prizes)\n",
        "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
        "    else:\n",
        "        n_prizes = torch.zeros(graph.num_nodes)\n",
        "\n",
        "    if topk_e > 0:\n",
        "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.edge_attr)\n",
        "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
        "\n",
        "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
        "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
        "        last_topk_e_value = topk_e\n",
        "        for k in range(topk_e):\n",
        "            indices = e_prizes == topk_e_values[k]\n",
        "            value = min((topk_e-k)/sum(indices), last_topk_e_value)\n",
        "            e_prizes[indices] = value\n",
        "            last_topk_e_value = value*(1-c)\n",
        "        # reduce the cost of the edges such that at least one edge is selected\n",
        "        cost_e = min(cost_e, e_prizes.max().item()*(1-c/2))\n",
        "    else:\n",
        "        e_prizes = torch.zeros(graph.num_edges)\n",
        "\n",
        "    costs = []\n",
        "    edges = []\n",
        "    vritual_n_prizes = []\n",
        "    virtual_edges = []\n",
        "    virtual_costs = []\n",
        "    mapping_n = {}\n",
        "    mapping_e = {}\n",
        "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
        "        prize_e = e_prizes[i]\n",
        "        if prize_e <= cost_e:\n",
        "            mapping_e[len(edges)] = i\n",
        "            edges.append((src, dst))\n",
        "            costs.append(cost_e - prize_e)\n",
        "        else:\n",
        "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
        "            mapping_n[virtual_node_id] = i\n",
        "            virtual_edges.append((src, virtual_node_id))\n",
        "            virtual_edges.append((virtual_node_id, dst))\n",
        "            virtual_costs.append(0)\n",
        "            virtual_costs.append(0)\n",
        "            vritual_n_prizes.append(prize_e - cost_e)\n",
        "\n",
        "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
        "    num_edges = len(edges)\n",
        "    if len(virtual_costs) > 0:\n",
        "        costs = np.array(costs+virtual_costs)\n",
        "        edges = np.array(edges+virtual_edges)\n",
        "\n",
        "    vertices, edges = pcst_fast(edges, prizes, costs, root, num_clusters, pruning, verbosity_level)\n",
        "\n",
        "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
        "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
        "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "    if len(virtual_vertices) > 0:\n",
        "        virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
        "        selected_edges = np.array(selected_edges+virtual_edges)\n",
        "\n",
        "    edge_index = graph.edge_index[:, selected_edges]\n",
        "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    n = textual_nodes.iloc[selected_nodes]\n",
        "    e = textual_edges.iloc[selected_edges]\n",
        "    desc = n.to_csv(index=False)+'\\n'+e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
        "\n",
        "    x = graph.x[selected_nodes]\n",
        "    edge_attr = graph.edge_attr[selected_edges]\n",
        "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
        "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
        "    edge_index = torch.LongTensor([src, dst])\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
        "\n",
        "    return data, desc"
      ],
      "metadata": {
        "id": "DxoyW5_U7NUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieve_pipeline.py"
      ],
      "metadata": {
        "id": "4fV8lnTL_xCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "#from retrieval import retrieval_via_pcst\n",
        "\n",
        "# Step 1 — Encode the query into an embedding\n",
        "def encode_query(query_text, query_encoder):\n",
        "    # query_encoder must be a model that outputs a vector from text\n",
        "    with torch.no_grad():\n",
        "        q_emb = query_encoder(query_text)  # returns a (feature_dim,) tensor\n",
        "    return q_emb\n",
        "\n",
        "# Step 2 — Prepare the graph (already assumed to be in PyG Data format)\n",
        "def prepare_graph(graph_data):\n",
        "    # graph_data must be a torch_geometric.data.Data object\n",
        "    return graph_data\n",
        "\n",
        "# Step 3 — Prepare textual node and edge tables\n",
        "def prepare_textual_info(graph_data):\n",
        "    # Dummy example: create node and edge DataFrames\n",
        "    textual_nodes = pd.DataFrame({'node_feature': graph_data.x.tolist()})\n",
        "    textual_edges = pd.DataFrame({\n",
        "        'src': graph_data.edge_index[0].tolist(),\n",
        "        'dst': graph_data.edge_index[1].tolist(),\n",
        "        'edge_attr': graph_data.edge_attr.tolist()\n",
        "    })\n",
        "    return textual_nodes, textual_edges\n",
        "\n",
        "# Step 4 — Retrieval via PCST\n",
        "def retrieve_subgraph(query_text, query_encoder, graph_data):\n",
        "    q_emb = encode_query(query_text, query_encoder)\n",
        "    textual_nodes, textual_edges = prepare_textual_info(graph_data)\n",
        "    subgraph, description = retrieval_via_pcst(\n",
        "        graph=graph_data,\n",
        "        q_emb=q_emb,\n",
        "        textual_nodes=textual_nodes,\n",
        "        textual_edges=textual_edges,\n",
        "        topk=3,\n",
        "        topk_e=3,\n",
        "        cost_e=0.5\n",
        "    )\n",
        "    return subgraph, description\n",
        "\n",
        "# Example usage:\n",
        "# query_encoder = ... (your trained model or any text embedder)\n",
        "# graph_data = ... (your loaded graph)\n",
        "# subgraph, desc = retrieve_subgraph(\"example query\", query_encoder, graph_data)\n"
      ],
      "metadata": {
        "id": "oXBl4c8S_OxP"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}