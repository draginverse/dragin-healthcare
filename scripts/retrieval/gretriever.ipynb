{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH5ozNUlszi+s+GFjlkyqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/draginverse/dragin-healthcare/blob/feature%2Fg-retriever/scripts/retrieval/gretriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pcst_fast\n",
        "!pip install torch_scatter -f https://data.pyg.org/\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6hUhz5AASGs",
        "outputId": "49fbbb2c-ea21-4dd0-826f-86ef4ed247db",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting pcst_fast\n",
            "  Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting pybind11>=2.1.0 (from pcst_fast)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pcst_fast-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11, pcst_fast\n",
            "Successfully installed pcst_fast-1.0.10 pybind11-2.13.6\n",
            "Looking in links: https://data.pyg.org/\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch_scatter\n",
            "  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=4ed8fd52ccf8fbfbdd4bb96113309f747726828d64dbdf24329d843237d5244d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "Successfully built torch_scatter\n",
            "Installing collected packages: torch_scatter\n",
            "Successfully installed torch_scatter-2.1.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simulate graphs input"
      ],
      "metadata": {
        "id": "YRSVHQhphrbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_graphs = [\n",
        "    [\n",
        "        (\"asthma\", \"caused_by\", \"allergens\"),\n",
        "        (\"inhaler\", \"treats\", \"asthma\"),\n",
        "        (\"asthma\", \"symptom\", \"shortness of breath\")\n",
        "    ],\n",
        "    [\n",
        "        (\"copd\", \"risk_factor\", \"smoking\"),\n",
        "        (\"oxygen therapy\", \"treats\", \"copd\"),\n",
        "        (\"copd\", \"symptom\", \"chronic cough\")\n",
        "    ],\n",
        "    [\n",
        "        (\"bronchitis\", \"caused_by\", \"virus\"),\n",
        "        (\"bronchitis\", \"symptom\", \"chest discomfort\"),\n",
        "        (\"rest\", \"helps_with\", \"bronchitis\")\n",
        "    ],\n",
        "    [\n",
        "        (\"pneumonia\", \"caused_by\", \"bacteria\"),\n",
        "        (\"antibiotics\", \"treats\", \"pneumonia\"),\n",
        "        (\"pneumonia\", \"symptom\", \"fever\")\n",
        "    ],\n",
        "    [\n",
        "        (\"covid-19\", \"affects\", \"lungs\"),\n",
        "        (\"vaccine\", \"prevents\", \"covid-19\"),\n",
        "        (\"covid-19\", \"symptom\", \"loss of smell\")\n",
        "    ]\n",
        "]\n"
      ],
      "metadata": {
        "id": "r5B2t8rAjyVr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform graphs to pyg (needed for the encoder)"
      ],
      "metadata": {
        "id": "sDfuGBlTPmjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "# 1. Load the MiniLM model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 2. Text to embedding function (optimized for MiniLM)\n",
        "def text2embedding(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                      max_length=128, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use mean pooling instead of CLS token for better performance\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu()\n",
        "    return embeddings\n",
        "\n",
        "# 3. Graph transformation function\n",
        "def transform_graphs_to_pyg(triple_graphs):\n",
        "    pyg_graphs = []\n",
        "\n",
        "    for triples in triple_graphs:\n",
        "        # Extract all node names\n",
        "        node_names = set()\n",
        "        for src, _, dst in triples:\n",
        "            node_names.update([src.lower(), dst.lower()])\n",
        "        node_names = sorted(node_names)\n",
        "        node_map = {name: idx for idx, name in enumerate(node_names)}\n",
        "\n",
        "        # Edge list and texts\n",
        "        edge_list = []\n",
        "        edge_texts = []\n",
        "        for src, rel, dst in triples:\n",
        "            edge_list.append([node_map[src.lower()], node_map[dst.lower()]])\n",
        "            edge_texts.append(f\"relation: {rel}\")\n",
        "\n",
        "        # Node embeddings\n",
        "        node_texts = [f\"node: {name}\" for name in node_names]\n",
        "        node_embeddings = text2embedding(node_texts)\n",
        "        edge_embeddings = text2embedding(edge_texts) if edge_texts else torch.zeros(0, 384)\n",
        "\n",
        "        # Create PyG graph\n",
        "        pyg_graph = Data(\n",
        "            x=node_embeddings,\n",
        "            edge_index=torch.tensor(edge_list).t().contiguous(),\n",
        "            edge_attr=edge_embeddings,\n",
        "            num_nodes=len(node_names)\n",
        "        )\n",
        "        pyg_graphs.append(pyg_graph)\n",
        "\n",
        "    return pyg_graphs\n",
        "\n",
        "# ======== Verify consistency ===============\n",
        "# Transform the graphs\n",
        "pyg_graphs = transform_graphs_to_pyg(toy_graphs)\n",
        "\n",
        "# Create a batch of graphs for processing\n",
        "toy_graph_batch = Batch.from_data_list(pyg_graphs)\n",
        "\n",
        "# Print information about the first graph\n",
        "print(\"First graph in PyG format:\")\n",
        "print(pyg_graphs[0])\n",
        "#print(\"\\nNode mapping:\", {name: idx for idx, name in enumerate(node_encoder.classes_)})\n",
        "#print(\"Edge type mapping:\", {name: idx for idx, name in enumerate(edge_type_encoder.classes_)})\n",
        "print(\"\\nBatch information:\")\n",
        "print(toy_graph_batch)\n",
        "print(\"Batch vector:\", toy_graph_batch.batch)\n",
        "\n",
        "print(\"Total nodes:\", toy_graph_batch.num_nodes)\n",
        "print(\"Batch vector max index:\", toy_graph_batch.batch.max())\n",
        "print(\"Batch vector length:\", len(toy_graph_batch.batch))\n",
        "\n",
        "assert toy_graph_batch.batch.max() < len(pyg_graphs), \"Batch indices exceed graph count\"\n",
        "assert len(toy_graph_batch.batch) == toy_graph_batch.num_nodes, \"Batch vector length mismatch\""
      ],
      "metadata": {
        "id": "xVglUMKT1quZ",
        "outputId": "1c45d548-fb9a-4734-918a-6bb977f473ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First graph in PyG format:\n",
            "Data(x=[4, 384], edge_index=[2, 3], edge_attr=[3, 384], num_nodes=4)\n",
            "\n",
            "Batch information:\n",
            "DataBatch(x=[20, 384], edge_index=[2, 15], edge_attr=[15, 384], num_nodes=20, batch=[20], ptr=[6])\n",
            "Batch vector: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])\n",
            "Total nodes: 20\n",
            "Batch vector max index: tensor(4)\n",
            "Batch vector length: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gnn.py"
      ],
      "metadata": {
        "id": "XJ1J2eLj_d-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TransformerConv, GATConv\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GCN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=-1):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(in_channels=in_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=hidden_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(TransformerConv(in_channels=hidden_channels, out_channels=out_channels//num_heads, heads=num_heads, edge_dim=in_channels, dropout=dropout,))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index=adj_t, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, num_heads=4):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GATConv(hidden_channels, out_channels, heads=num_heads, concat=False))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x,edge_index=edge_index, edge_attr=edge_attr)\n",
        "        return x, edge_attr\n",
        "\n",
        "\n",
        "load_gnn_model = {\n",
        "    'gcn': GCN,\n",
        "    'gat': GAT,\n",
        "    'gt': GraphTransformer,\n",
        "}"
      ],
      "metadata": {
        "id": "OKE0QK-R55bL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# graph_encoder.py (embeds graphs)"
      ],
      "metadata": {
        "id": "fzfEgi_l_mOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W9iAVCzK2cdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter\n",
        "#from gnn import load_gnn_model\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.graph_encoder = load_gnn_model[args.gnn_model_name](\n",
        "            in_channels=args.gnn_in_dim,\n",
        "            out_channels=args.gnn_hidden_dim,\n",
        "            hidden_channels=args.gnn_hidden_dim,\n",
        "            num_layers=args.gnn_num_layers,\n",
        "            dropout=args.gnn_dropout,\n",
        "            num_heads=args.gnn_num_heads,\n",
        "        )\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(args.gnn_hidden_dim, 2048),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(2048, 4096),\n",
        "        )\n",
        "\n",
        "    def encode(self, graphs):\n",
        "        graphs = graphs.to(next(self.parameters()).device)\n",
        "        n_embeds, _ = self.graph_encoder(graphs.x, graphs.edge_index.long(), graphs.edge_attr)\n",
        "        g_embeds = scatter(n_embeds, graphs.batch, dim=0, reduce='mean')\n",
        "        projected_embeds = self.projector(g_embeds)\n",
        "        return projected_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pcst_retrieval.py"
      ],
      "metadata": {
        "id": "yShIE19f_sEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pcst_fast import pcst_fast\n",
        "from torch_geometric.data.data import Data\n",
        "\n",
        "\n",
        "def retrieval_via_pcst(graph, q_emb, textual_nodes, textual_edges, topk=3, topk_e=3, cost_e=0.5):\n",
        "    c = 0.01\n",
        "    if len(textual_nodes) == 0 or len(textual_edges) == 0:\n",
        "        desc = textual_nodes.to_csv(index=False) + '\\n' + textual_edges.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "        graph = Data(x=graph.x, edge_index=graph.edge_index, edge_attr=graph.edge_attr, num_nodes=graph.num_nodes)\n",
        "        return graph, desc\n",
        "\n",
        "    root = -1  # unrooted\n",
        "    num_clusters = 1\n",
        "    pruning = 'gw'\n",
        "    verbosity_level = 0\n",
        "    if topk > 0:\n",
        "        n_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.x)\n",
        "        topk = min(topk, graph.num_nodes)\n",
        "        _, topk_n_indices = torch.topk(n_prizes, topk, largest=True)\n",
        "\n",
        "        n_prizes = torch.zeros_like(n_prizes)\n",
        "        n_prizes[topk_n_indices] = torch.arange(topk, 0, -1).float()\n",
        "    else:\n",
        "        n_prizes = torch.zeros(graph.num_nodes)\n",
        "\n",
        "    if topk_e > 0:\n",
        "        e_prizes = torch.nn.CosineSimilarity(dim=-1)(q_emb, graph.edge_attr)\n",
        "        topk_e = min(topk_e, e_prizes.unique().size(0))\n",
        "\n",
        "        topk_e_values, _ = torch.topk(e_prizes.unique(), topk_e, largest=True)\n",
        "        e_prizes[e_prizes < topk_e_values[-1]] = 0.0\n",
        "        last_topk_e_value = topk_e\n",
        "        for k in range(topk_e):\n",
        "            indices = e_prizes == topk_e_values[k]\n",
        "            value = min((topk_e-k)/sum(indices), last_topk_e_value)\n",
        "            e_prizes[indices] = value\n",
        "            last_topk_e_value = value*(1-c)\n",
        "        # reduce the cost of the edges such that at least one edge is selected\n",
        "        cost_e = min(cost_e, e_prizes.max().item()*(1-c/2))\n",
        "    else:\n",
        "        e_prizes = torch.zeros(graph.num_edges)\n",
        "\n",
        "    costs = []\n",
        "    edges = []\n",
        "    vritual_n_prizes = []\n",
        "    virtual_edges = []\n",
        "    virtual_costs = []\n",
        "    mapping_n = {}\n",
        "    mapping_e = {}\n",
        "    for i, (src, dst) in enumerate(graph.edge_index.T.numpy()):\n",
        "        prize_e = e_prizes[i]\n",
        "        if prize_e <= cost_e:\n",
        "            mapping_e[len(edges)] = i\n",
        "            edges.append((src, dst))\n",
        "            costs.append(cost_e - prize_e)\n",
        "        else:\n",
        "            virtual_node_id = graph.num_nodes + len(vritual_n_prizes)\n",
        "            mapping_n[virtual_node_id] = i\n",
        "            virtual_edges.append((src, virtual_node_id))\n",
        "            virtual_edges.append((virtual_node_id, dst))\n",
        "            virtual_costs.append(0)\n",
        "            virtual_costs.append(0)\n",
        "            vritual_n_prizes.append(prize_e - cost_e)\n",
        "\n",
        "    prizes = np.concatenate([n_prizes, np.array(vritual_n_prizes)])\n",
        "    num_edges = len(edges)\n",
        "    if len(virtual_costs) > 0:\n",
        "        costs = np.array(costs+virtual_costs)\n",
        "        edges = np.array(edges+virtual_edges)\n",
        "\n",
        "    vertices, edges = pcst_fast(edges, prizes, costs, root, num_clusters, pruning, verbosity_level)\n",
        "\n",
        "    selected_nodes = vertices[vertices < graph.num_nodes]\n",
        "    selected_edges = [mapping_e[e] for e in edges if e < num_edges]\n",
        "    virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "    if len(virtual_vertices) > 0:\n",
        "        virtual_vertices = vertices[vertices >= graph.num_nodes]\n",
        "        virtual_edges = [mapping_n[i] for i in virtual_vertices]\n",
        "        selected_edges = np.array(selected_edges+virtual_edges)\n",
        "\n",
        "    edge_index = graph.edge_index[:, selected_edges]\n",
        "    selected_nodes = np.unique(np.concatenate([selected_nodes, edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    n = textual_nodes.iloc[selected_nodes]\n",
        "    e = textual_edges.iloc[selected_edges]\n",
        "    desc = n.to_csv(index=False)+'\\n'+e.to_csv(index=False, columns=['src', 'edge_attr', 'dst'])\n",
        "\n",
        "    mapping = {n: i for i, n in enumerate(selected_nodes.tolist())}\n",
        "\n",
        "    x = graph.x[selected_nodes]\n",
        "    edge_attr = graph.edge_attr[selected_edges]\n",
        "    src = [mapping[i] for i in edge_index[0].tolist()]\n",
        "    dst = [mapping[i] for i in edge_index[1].tolist()]\n",
        "    edge_index = torch.LongTensor([src, dst])\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(selected_nodes))\n",
        "\n",
        "    return data, desc"
      ],
      "metadata": {
        "id": "DxoyW5_U7NUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compute graph embeddings & retrieve the most relevant graph:"
      ],
      "metadata": {
        "id": "_7VjFFCzulqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from types import SimpleNamespace\n",
        "#from src.model.graph_encoder import GraphEncoder\n",
        "#from src.model.gnn import load_gnn_model\n",
        "\n",
        "def retrieve_relevant_graph(toy_graphs, query):\n",
        "    # 1. Load text encoder\n",
        "    text_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # Or another model\n",
        "\n",
        "    # 2. Define GNN parameters\n",
        "    graph_encoder_args = SimpleNamespace(\n",
        "        gnn_model_name=\"gat\",\n",
        "        gnn_in_dim=384,        # SBERT dim\n",
        "        gnn_hidden_dim=128,\n",
        "        gnn_num_layers=2,\n",
        "        gnn_dropout=0.1,\n",
        "        gnn_num_heads=4,       # Needed for GAT\n",
        "    )\n",
        "\n",
        "    # 3. Create the graph encoder\n",
        "    graph_encoder = GraphEncoder(graph_encoder_args)\n",
        "\n",
        "    graph_encoder.eval()  # Disable dropout during inference\n",
        "\n",
        "    # 4. Encode all graphs into fixed-size graph representations\n",
        "    '''graph_reprs = []\n",
        "    for graph in toy_graph_batch:\n",
        "        with torch.no_grad():\n",
        "            repr = graph_encoder.encode(graph)  # add batch dimension if needed\n",
        "            graph_reprs.append(repr.squeeze(0))  # shape: [hidden_dim]\n",
        "\n",
        "    graph_reprs = torch.stack(graph_reprs)  # shape: [num_graphs, hidden_dim]'''\n",
        "    # ===\n",
        "    # 4.1. Transform all graphs to PyG format\n",
        "    pyg_graphs = transform_graphs_to_pyg(toy_graphs)\n",
        "\n",
        "    # 4.2. Create one batch containing all graphs\n",
        "    batch = Batch.from_data_list(pyg_graphs)\n",
        "\n",
        "    # 4.3. Encode all graphs in one forward pass (most efficient)\n",
        "    with torch.no_grad():\n",
        "        graph_reprs = graph_encoder.encode(batch)\n",
        "    # ===\n",
        "    '''# Test each graph separately\n",
        "    for i, data in enumerate(pyg_graphs):\n",
        "        try:\n",
        "            single_batch = Batch.from_data_list([data])\n",
        "            with torch.no_grad():\n",
        "                graph_encoder.encode(single_batch)\n",
        "            print(f\"Graph {i} processed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in graph {i}: {str(e)}\")'''\n",
        "\n",
        "    # 5. Encode the query\n",
        "    q_enc = text_encoder.encode(query, convert_to_tensor=True) # shape: [hidden_dim]\n",
        "    q_emb = q_enc.clone().detach()\n",
        "    # Process query with projection\n",
        "    text_projection = nn.Linear(384, 4096)\n",
        "    with torch.no_grad():\n",
        "        q_emb = text_encoder.encode(query, convert_to_tensor=True)\n",
        "        q_emb = text_projection(q_emb)  # Now shape: [4096]\n",
        "\n",
        "    # 6. Compare and select the most similar graph\n",
        "    sims = torch.nn.functional.cosine_similarity(q_emb.unsqueeze(0), graph_reprs)  # shape: [num_graphs]\n",
        "    top_idx = torch.argmax(sims).item()\n",
        "\n",
        "    # 7. Get the most relevant graph\n",
        "    selected_graph = toy_graph_batch[top_idx]\n",
        "    #selected_node_texts = toy_node_texts[top_idx]\n",
        "    #selected_edge_texts = toy_edge_texts[top_idx]\n",
        "\n",
        "    return selected_graph, top_idx, sims\n"
      ],
      "metadata": {
        "id": "D_LCx18Zumdy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieve once"
      ],
      "metadata": {
        "id": "A6v3BdbZOqxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how can i treat asthma?\"\n",
        "# Get the most similar graph\n",
        "selected_graph, top_idx, sims = retrieve_relevant_graph(toy_graphs, query)\n",
        "#print(f\"Most relevant graph index: {top_idx}\")\n",
        "#print(selected_graph)\n",
        "\n",
        "matched_graph = toy_graphs[top_idx]\n",
        "# Print with formatting\n",
        "print(\"=== Most Similar Graph ===\")\n",
        "print(f\"Match Score: {sims[top_idx]:.3f}\")\n",
        "print(f\"Graph : {matched_graph}\")"
      ],
      "metadata": {
        "id": "7pd_TATIOfTF",
        "outputId": "e2f76be6-aa1c-434e-9145-5e11fb8123f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most relevant graph index: 3\n",
            "Data(x=[4, 384], edge_index=[2, 3], edge_attr=[3, 384], num_nodes=4)\n",
            "=== Most Similar Graph ===\n",
            "Match Score: 0.019\n",
            "Graph : [('pneumonia', 'caused_by', 'bacteria'), ('antibiotics', 'treats', 'pneumonia'), ('pneumonia', 'symptom', 'fever')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieve 50x for testing"
      ],
      "metadata": {
        "id": "IX_vO8c_OmaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "query = \"how can i treat asthma?\"\n",
        "number_of_retrievals = 50\n",
        "\n",
        "# Run retrievals\n",
        "all_results = []\n",
        "for _ in range(number_of_retrievals):\n",
        "    _, top_idx, sims = retrieve_relevant_graph(toy_graphs, query)\n",
        "    all_results.append((top_idx, sims[top_idx].item()))\n",
        "\n",
        "# Analyze\n",
        "stats = defaultdict(list)\n",
        "for idx, score in all_results:\n",
        "    stats[idx].append(score)\n",
        "\n",
        "print(\"\\n=== Detailed Statistics ===\")\n",
        "for idx in sorted(stats.keys(), key=lambda x: -np.mean(stats[x])):\n",
        "    scores = stats[idx]\n",
        "    graph = toy_graphs[idx]\n",
        "    print(f\"\\nGraph {idx}:\")\n",
        "    print(f\"  Frequency: {len(scores)}/{number_of_retrievals}\")\n",
        "    print(f\"  Avg Score: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
        "    print(f\"  Preview : {graph[0]}...\")"
      ],
      "metadata": {
        "id": "YUvKV2ijJbpV",
        "outputId": "21574cac-2b89-4d91-fd09-88a51c047dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Detailed Statistics ===\n",
            "\n",
            "Graph 0:\n",
            "  Frequency: 4/50\n",
            "  Avg Score: 0.007 ± 0.009\n",
            "  Preview : ('asthma', 'caused_by', 'allergens')...\n",
            "\n",
            "Graph 3:\n",
            "  Frequency: 19/50\n",
            "  Avg Score: 0.001 ± 0.018\n",
            "  Preview : ('pneumonia', 'caused_by', 'bacteria')...\n",
            "\n",
            "Graph 4:\n",
            "  Frequency: 7/50\n",
            "  Avg Score: 0.000 ± 0.016\n",
            "  Preview : ('covid-19', 'affects', 'lungs')...\n",
            "\n",
            "Graph 1:\n",
            "  Frequency: 12/50\n",
            "  Avg Score: -0.001 ± 0.020\n",
            "  Preview : ('copd', 'risk_factor', 'smoking')...\n",
            "\n",
            "Graph 2:\n",
            "  Frequency: 8/50\n",
            "  Avg Score: -0.005 ± 0.014\n",
            "  Preview : ('bronchitis', 'caused_by', 'virus')...\n"
          ]
        }
      ]
    }
  ]
}